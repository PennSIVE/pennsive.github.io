[
  {
    "objectID": "rpackages.html",
    "href": "rpackages.html",
    "title": "R Packages",
    "section": "",
    "text": "When writing a methodological paper, it is a good idea to make your code publicly available, since it will be much easier for others to use your method on their own data. One way to do this is to create an R package.\nR packages can be complicated, but the easiest way to get started is to make a bare-bones, minimal R package and then add complexity. This article will show you how!\nNote: These instructions are blatantly stolen from Hilary Parker’s article, Karl Broman’s article, Andrew’s slides, and Danni’s teaching materials from BSTA 670. Everything was tested on R version 4.1.3.\n\n\n\n\ndevtools::create()\ndevtools::document()\ndevtools::install()\n\n\n\n\nOur goal is to make a minimal package called pennsive_stuff, consisting of 2 R functions. Suppose these functions are located in two .R files, nl.R and plot_dist.R, containing the following:\n\nnl(): a function that prints new lines in R Markdown documents.\n\n# Contents of nl.R\nnl &lt;- function(){\n  cat(\"  \\n\")\n  cat(\"  \\n\")\n}\n\nplot_dist(): a function that plots the distribution of a variable in a dataset.\n\n# Contents of plot_dist.R\nplot_dist &lt;- function(dat, var_name){\n  stopifnot(var_name %in% colnames(dat))\n  stopifnot(class(dat[,var_name]) == \"numeric\")\n  \n  p &lt;- ggplot2::ggplot(dat) +\n    ggplot2::geom_histogram(ggplot2::aes_string(x = var_name), bins = 30, \n                            fill = wesanderson::wes_palettes$GrandBudapest2[1]) +\n    ggplot2::theme_minimal() +\n    ggplot2::labs(y = \"Count\", x = var_name)\n  \n  return(p)\n}\nNote that plot_dist() contains dependencies from the ggplot and wesanderson packages.\n\n\n# Run in R\ninstall.packages(c(\"available\", \"usethis\", \"testthat\", \"pkgdown\", \"roxygen2\", \"devtools\", \"covr\"))\n(But how were those packages created?? Ooooo!)\nTo make a super minimal package, you really only need roxygen2 and devtools.\n\n\n\nThis will also check if the name has been taken on CRAN and other sites.\n# Run in R\n# Will look up your package name on CRAN, Bioconductor, and Github\n# Will also check the meaning of your package name on several websites\navailable::available(name = \"pennsive_stuff\", browse = FALSE)\n\n\n\n# Run in R\ndir.package &lt;- [the directory on your computer that you'd like everything to be in]\nsetwd(dir.package)\ndevtools::create(\"pennsive_stuff\")\nThis will automatically create a folder in your working directory called pennsive_stuff:\n# Run in R\nlist.files(\"pennsive_stuff\")\n\nDESCRIPTION has details about your package, which can be edited in an app like TextEdit (Mac) or WordPad (Windows).\nNAMESPACE has the names of the objects in your package that will be made available upon library()ing it. It’s empty right now!\nR is a folder that will hold all of your function .R files.\n\n\n\n\nEach function that you want to include in the package should be in an .R file and placed in the folder pennsive_stuff/R that was created in Step 2. You can have one function per .R file, or organize multiple functions in one .R file. It doesn’t mattter!\nLet’s put the .R files, nl.R and plot_dist.R–which contain the code for nl() and plot_dist() respectively–into pennsive_stuff/R:\n# Run in R. \n# Ensure that the two files, nl.R and plot_dist.R are in pennsive_stuff/R\nlist.files(file.path(\"pennsive_stuff\", \"R\"))\n\n\n\nYou know the nice notes that you see when you do e.g. ?sample? We too can have nice things!\nThe most tedious parts of documentation are taken care of through the Roxygen2 package. All you have to do is add special comments at the start of the .R files that you just copied to /pennsive_stuff/R.\nRoxygen2 documentation is just extra lines added before the functions in the .R files and they look roughly like this:\n#' Short summary of the function\n#'\n#' Description of the function\n#'\n#' @param param_name1 description of parameter/input 1\n#' @param param_name1 description of parameter/input 1\n#'\n#' @return description of what is returned\n#'\n#' @examples\n#' nl()\n#'\n#' @export\n\nThe final line @export adds this function to the NAMESPACE file, so that is accessible (i.e., via pennsive_stuff::nl() or after calling library). Otherwise, you would have to access it through the ::: operator, à la pennsive_stuff:::nl().\nThere are many options for formatting these things (e.g., adding links, equations): see https://roxygen2.r-lib.org/articles/formatting.html.\n\nFor example, after adding documentation to nl.R, the file would look like:\n# Contents of nl.R\n\n#' Print new line in HTML files\n#'\n#' This function prints a new line in HTML files, which is useful when printing other things such as headers or plots.\n#'\n#' @return Returns nothing except some spaces!!\n#'\n#' @examples\n#' nl()\n#'\n#' @export\n\nnl &lt;- function(){\n  cat(\"  \\n\")\n  cat(\"  \\n\")\n}\nFor plot_dist.R, because the function depends on the ggplot2 and wesanderson R packages, we would have an additional line for @import (imports everything in the namespace) and @importFrom (imports only a particular thing).\n# Contents of plot_dist.R\n\n#' Plot Histogram\n#'\n#' Plot the histogram of a numeric variable within a data frame\n#'\n#' @param dat a data.frame containing the data to plot.\n#' @param var_name a character vector of the column name in the data.\n#'\n#' @return a histogram plot.\n#' @import ggplot2\n#' @importFrom wesanderson wes_palettes\n#'\n#' @examples\n#' plot_dist(dat = iris, var_name = \"Petal.Length\")\n#'\n#' @export\n\nplot_dist &lt;- function(dat, var_name){\n  stopifnot(var_name %in% colnames(dat))\n  stopifnot(class(dat[,var_name]) == \"numeric\")\n  \n  p &lt;- ggplot2::ggplot(dat) +\n    ggplot2::geom_histogram(ggplot2::aes_string(x = var_name), bins = 30, \n                            fill = wesanderson::wes_palettes$GrandBudapest2[1]) +\n    ggplot2::theme_minimal() +\n    ggplot2::labs(y = \"Count\", x = var_name)\n  \n  return(p)\n}\n\n\n# Run in R\nsetwd(file.path(dir.package, \"pennsive_stuff\"))\ndevtools::document()\nAfter running document(), you will see some .Rd files automatically generated for you in the man folder:\n# Run in R\nlist.files(\"man\")\n\n\n\n\nThat was it! Easy, right? Now let’s install the package while in the directory containing the pennsive_stuff folder:\n# Run in R\nsetwd(dir.package)\ndevtools::install(\"pennsive_stuff\")\nNow you have the package!! You can call the library command from anywhere (on your computer) now.\n# Run in R\nlibrary(pennsive_stuff)\nnl()\nplot_dist(dat = iris, var_name = \"Petal.Length\")\nAnd you can see the results of the Roxygen2 documentation here:\n# Run in R\n?nl()\n?plot_dist()\nGreat job!\n\n\n\n\nYou have a basic R package; everything else is gravy. Here are some more things you can do with your package:\n\nInclude data.\nUpload to Github, which allows your package to be installed via devtools::install_github().\nUplaod to CRAN or Bioconductor.\n\n\n\n\nSome packages come with data (e.g. refund::DTI) that accompany the functions. Suppose we also want the package to contain the dataset iris2.\nIn the package directory, create a folder called data:\n# Run in R\nif (!dir.exists(file.path(dir.package, \"pennsive_stuff/data\"))) dir.create(file.path(dir.package, \"pennsive_stuff/data\"))\nThe easiest way to save the data in the data folder is to use usethis::usedata():\n# Run in R\nset.seed(382349)\niris2 = iris[sample(x = 1:nrow(iris), size = 100, replace = TRUE),]\n\nsetwd(file.path(dir.package, \"pennsive_stuff\"))\nusethis::use_data(iris2, iris2)\nThen, you’ll need to create an R file named data.R in the R folder, where your other R functions are stored.\nSimilar to the Roxygen2 documentation for functions, the data.R file needs to contain the following:\n# Contents of data.R\n\n#' Edgar Anderson's Iris Data\n#'\n#' This famous (Fisher's or Anderson's) iris data set gives the measurements in \n#' centimeters of the variables sepal length and width and petal length and width, \n#' respectively, for 50 flowers from each of 3 species of iris. The species are \n#' Iris setosa, versicolor, and virginica.\n#'\n#' @docType data\n#'\n#' @usage data(iris2)\n#'\n#' @format \\code{\"iris\"} is a data frame with 150 cases (rows) and 5 variables\n#'  (columns) named Sepal.Length, Sepal.Width, Petal.Length, Petal.Width, and Species.\n#'\n#' @keywords datasets\n#'\n#' @references Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988) \n#' The New S Language. Wadsworth & Brooks/Cole. \n\n#'\n#' @source \\href{https://phenome.jax.org/projects/Moore1b}{QTL Archive}\n#'\n#' @examples\n#' data(iris2)\n#' plot_dist(dat = iris2, var_name = \"Petal.Width\")\n\"iris2\"\nNow you can re-document and re-install the package and get the iris2 data using data(iris2)!\n# Run in R\n\ndevtools::document()\n\nsetwd(file.path(dir.package))\ndevtools::install(\"pennsive_stuff\")\nlibrary(pennsive_stuff)\n\ndata(iris2)\nplot_dist(dat = iris2, var_name = \"Sepal.Length\")\n?iris2\n\n\n\nKeep track of the versions of your package using Git. Once you set up a Git repo in your local drive, it’s easy to share it on Github (as easy as anything with Git/Github, anyways…)\n\n\nFirst, cd to the package folder /pennsive_stuff/ in your Terminal and type\n# Run in terminal\ngit init\ngit add .\ngit commit\nIf you are using RStudio and have opened the project pennsive_stuff.Rproj, you should see a “Git” tab in the Environment, History, Connections tab.\n\n\n\nNext, go to Github and make a new repo with the name as this package, pennsive_stuff. Leave the box for README unchecked!\nYou may need to configure RStudio access to your Github account via a personal access token. Easy instructions here: https://gist.github.com/Z3tt/3dab3535007acf108391649766409421.\n\n\n\nIn terminal,\n# Run in terminal\ngit remote add origin https://github.com/[your github-id]/pennsive_stuff\ngit push origin HEAD:main\nFor more details, see https://andy1764.github.io/Making%20an%20R%20Package/Making-an-R-Package.html#47 and https://kbroman.org/pkg_primer/pages/github.html.\n\n\n\n\nThere are many hoops to jump through to put your package on CRAN. CRAN is maintained by volunteers, who manually check your R package (especially if it’s the first submission) in addition to all of the automated checks. https://kbroman.org/pkg_primer/pages/cran.html.\nYou can also put your package on Bioconductor, which is reportedly easier: https://bioconductor.org/developers/package-submission/."
  },
  {
    "objectID": "rpackages.html#making-r-packages",
    "href": "rpackages.html#making-r-packages",
    "title": "R Packages",
    "section": "",
    "text": "When writing a methodological paper, it is a good idea to make your code publicly available, since it will be much easier for others to use your method on their own data. One way to do this is to create an R package.\nR packages can be complicated, but the easiest way to get started is to make a bare-bones, minimal R package and then add complexity. This article will show you how!\nNote: These instructions are blatantly stolen from Hilary Parker’s article, Karl Broman’s article, Andrew’s slides, and Danni’s teaching materials from BSTA 670. Everything was tested on R version 4.1.3."
  },
  {
    "objectID": "rpackages.html#tldr-you-can-make-an-r-package-with-3-lines-of-code",
    "href": "rpackages.html#tldr-you-can-make-an-r-package-with-3-lines-of-code",
    "title": "R Packages",
    "section": "",
    "text": "devtools::create()\ndevtools::document()\ndevtools::install()"
  },
  {
    "objectID": "rpackages.html#example-pennsive_stuff",
    "href": "rpackages.html#example-pennsive_stuff",
    "title": "R Packages",
    "section": "",
    "text": "Our goal is to make a minimal package called pennsive_stuff, consisting of 2 R functions. Suppose these functions are located in two .R files, nl.R and plot_dist.R, containing the following:\n\nnl(): a function that prints new lines in R Markdown documents.\n\n# Contents of nl.R\nnl &lt;- function(){\n  cat(\"  \\n\")\n  cat(\"  \\n\")\n}\n\nplot_dist(): a function that plots the distribution of a variable in a dataset.\n\n# Contents of plot_dist.R\nplot_dist &lt;- function(dat, var_name){\n  stopifnot(var_name %in% colnames(dat))\n  stopifnot(class(dat[,var_name]) == \"numeric\")\n  \n  p &lt;- ggplot2::ggplot(dat) +\n    ggplot2::geom_histogram(ggplot2::aes_string(x = var_name), bins = 30, \n                            fill = wesanderson::wes_palettes$GrandBudapest2[1]) +\n    ggplot2::theme_minimal() +\n    ggplot2::labs(y = \"Count\", x = var_name)\n  \n  return(p)\n}\nNote that plot_dist() contains dependencies from the ggplot and wesanderson packages.\n\n\n# Run in R\ninstall.packages(c(\"available\", \"usethis\", \"testthat\", \"pkgdown\", \"roxygen2\", \"devtools\", \"covr\"))\n(But how were those packages created?? Ooooo!)\nTo make a super minimal package, you really only need roxygen2 and devtools.\n\n\n\nThis will also check if the name has been taken on CRAN and other sites.\n# Run in R\n# Will look up your package name on CRAN, Bioconductor, and Github\n# Will also check the meaning of your package name on several websites\navailable::available(name = \"pennsive_stuff\", browse = FALSE)\n\n\n\n# Run in R\ndir.package &lt;- [the directory on your computer that you'd like everything to be in]\nsetwd(dir.package)\ndevtools::create(\"pennsive_stuff\")\nThis will automatically create a folder in your working directory called pennsive_stuff:\n# Run in R\nlist.files(\"pennsive_stuff\")\n\nDESCRIPTION has details about your package, which can be edited in an app like TextEdit (Mac) or WordPad (Windows).\nNAMESPACE has the names of the objects in your package that will be made available upon library()ing it. It’s empty right now!\nR is a folder that will hold all of your function .R files.\n\n\n\n\nEach function that you want to include in the package should be in an .R file and placed in the folder pennsive_stuff/R that was created in Step 2. You can have one function per .R file, or organize multiple functions in one .R file. It doesn’t mattter!\nLet’s put the .R files, nl.R and plot_dist.R–which contain the code for nl() and plot_dist() respectively–into pennsive_stuff/R:\n# Run in R. \n# Ensure that the two files, nl.R and plot_dist.R are in pennsive_stuff/R\nlist.files(file.path(\"pennsive_stuff\", \"R\"))\n\n\n\nYou know the nice notes that you see when you do e.g. ?sample? We too can have nice things!\nThe most tedious parts of documentation are taken care of through the Roxygen2 package. All you have to do is add special comments at the start of the .R files that you just copied to /pennsive_stuff/R.\nRoxygen2 documentation is just extra lines added before the functions in the .R files and they look roughly like this:\n#' Short summary of the function\n#'\n#' Description of the function\n#'\n#' @param param_name1 description of parameter/input 1\n#' @param param_name1 description of parameter/input 1\n#'\n#' @return description of what is returned\n#'\n#' @examples\n#' nl()\n#'\n#' @export\n\nThe final line @export adds this function to the NAMESPACE file, so that is accessible (i.e., via pennsive_stuff::nl() or after calling library). Otherwise, you would have to access it through the ::: operator, à la pennsive_stuff:::nl().\nThere are many options for formatting these things (e.g., adding links, equations): see https://roxygen2.r-lib.org/articles/formatting.html.\n\nFor example, after adding documentation to nl.R, the file would look like:\n# Contents of nl.R\n\n#' Print new line in HTML files\n#'\n#' This function prints a new line in HTML files, which is useful when printing other things such as headers or plots.\n#'\n#' @return Returns nothing except some spaces!!\n#'\n#' @examples\n#' nl()\n#'\n#' @export\n\nnl &lt;- function(){\n  cat(\"  \\n\")\n  cat(\"  \\n\")\n}\nFor plot_dist.R, because the function depends on the ggplot2 and wesanderson R packages, we would have an additional line for @import (imports everything in the namespace) and @importFrom (imports only a particular thing).\n# Contents of plot_dist.R\n\n#' Plot Histogram\n#'\n#' Plot the histogram of a numeric variable within a data frame\n#'\n#' @param dat a data.frame containing the data to plot.\n#' @param var_name a character vector of the column name in the data.\n#'\n#' @return a histogram plot.\n#' @import ggplot2\n#' @importFrom wesanderson wes_palettes\n#'\n#' @examples\n#' plot_dist(dat = iris, var_name = \"Petal.Length\")\n#'\n#' @export\n\nplot_dist &lt;- function(dat, var_name){\n  stopifnot(var_name %in% colnames(dat))\n  stopifnot(class(dat[,var_name]) == \"numeric\")\n  \n  p &lt;- ggplot2::ggplot(dat) +\n    ggplot2::geom_histogram(ggplot2::aes_string(x = var_name), bins = 30, \n                            fill = wesanderson::wes_palettes$GrandBudapest2[1]) +\n    ggplot2::theme_minimal() +\n    ggplot2::labs(y = \"Count\", x = var_name)\n  \n  return(p)\n}\n\n\n# Run in R\nsetwd(file.path(dir.package, \"pennsive_stuff\"))\ndevtools::document()\nAfter running document(), you will see some .Rd files automatically generated for you in the man folder:\n# Run in R\nlist.files(\"man\")\n\n\n\n\nThat was it! Easy, right? Now let’s install the package while in the directory containing the pennsive_stuff folder:\n# Run in R\nsetwd(dir.package)\ndevtools::install(\"pennsive_stuff\")\nNow you have the package!! You can call the library command from anywhere (on your computer) now.\n# Run in R\nlibrary(pennsive_stuff)\nnl()\nplot_dist(dat = iris, var_name = \"Petal.Length\")\nAnd you can see the results of the Roxygen2 documentation here:\n# Run in R\n?nl()\n?plot_dist()\nGreat job!"
  },
  {
    "objectID": "rpackages.html#extras",
    "href": "rpackages.html#extras",
    "title": "R Packages",
    "section": "",
    "text": "You have a basic R package; everything else is gravy. Here are some more things you can do with your package:\n\nInclude data.\nUpload to Github, which allows your package to be installed via devtools::install_github().\nUplaod to CRAN or Bioconductor."
  },
  {
    "objectID": "rpackages.html#extras-1-including-data",
    "href": "rpackages.html#extras-1-including-data",
    "title": "R Packages",
    "section": "",
    "text": "Some packages come with data (e.g. refund::DTI) that accompany the functions. Suppose we also want the package to contain the dataset iris2.\nIn the package directory, create a folder called data:\n# Run in R\nif (!dir.exists(file.path(dir.package, \"pennsive_stuff/data\"))) dir.create(file.path(dir.package, \"pennsive_stuff/data\"))\nThe easiest way to save the data in the data folder is to use usethis::usedata():\n# Run in R\nset.seed(382349)\niris2 = iris[sample(x = 1:nrow(iris), size = 100, replace = TRUE),]\n\nsetwd(file.path(dir.package, \"pennsive_stuff\"))\nusethis::use_data(iris2, iris2)\nThen, you’ll need to create an R file named data.R in the R folder, where your other R functions are stored.\nSimilar to the Roxygen2 documentation for functions, the data.R file needs to contain the following:\n# Contents of data.R\n\n#' Edgar Anderson's Iris Data\n#'\n#' This famous (Fisher's or Anderson's) iris data set gives the measurements in \n#' centimeters of the variables sepal length and width and petal length and width, \n#' respectively, for 50 flowers from each of 3 species of iris. The species are \n#' Iris setosa, versicolor, and virginica.\n#'\n#' @docType data\n#'\n#' @usage data(iris2)\n#'\n#' @format \\code{\"iris\"} is a data frame with 150 cases (rows) and 5 variables\n#'  (columns) named Sepal.Length, Sepal.Width, Petal.Length, Petal.Width, and Species.\n#'\n#' @keywords datasets\n#'\n#' @references Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988) \n#' The New S Language. Wadsworth & Brooks/Cole. \n\n#'\n#' @source \\href{https://phenome.jax.org/projects/Moore1b}{QTL Archive}\n#'\n#' @examples\n#' data(iris2)\n#' plot_dist(dat = iris2, var_name = \"Petal.Width\")\n\"iris2\"\nNow you can re-document and re-install the package and get the iris2 data using data(iris2)!\n# Run in R\n\ndevtools::document()\n\nsetwd(file.path(dir.package))\ndevtools::install(\"pennsive_stuff\")\nlibrary(pennsive_stuff)\n\ndata(iris2)\nplot_dist(dat = iris2, var_name = \"Sepal.Length\")\n?iris2"
  },
  {
    "objectID": "rpackages.html#extras-2-version-control",
    "href": "rpackages.html#extras-2-version-control",
    "title": "R Packages",
    "section": "",
    "text": "Keep track of the versions of your package using Git. Once you set up a Git repo in your local drive, it’s easy to share it on Github (as easy as anything with Git/Github, anyways…)\n\n\nFirst, cd to the package folder /pennsive_stuff/ in your Terminal and type\n# Run in terminal\ngit init\ngit add .\ngit commit\nIf you are using RStudio and have opened the project pennsive_stuff.Rproj, you should see a “Git” tab in the Environment, History, Connections tab.\n\n\n\nNext, go to Github and make a new repo with the name as this package, pennsive_stuff. Leave the box for README unchecked!\nYou may need to configure RStudio access to your Github account via a personal access token. Easy instructions here: https://gist.github.com/Z3tt/3dab3535007acf108391649766409421.\n\n\n\nIn terminal,\n# Run in terminal\ngit remote add origin https://github.com/[your github-id]/pennsive_stuff\ngit push origin HEAD:main\nFor more details, see https://andy1764.github.io/Making%20an%20R%20Package/Making-an-R-Package.html#47 and https://kbroman.org/pkg_primer/pages/github.html."
  },
  {
    "objectID": "rpackages.html#extras-3-uploading-to-cranbioconductor",
    "href": "rpackages.html#extras-3-uploading-to-cranbioconductor",
    "title": "R Packages",
    "section": "",
    "text": "There are many hoops to jump through to put your package on CRAN. CRAN is maintained by volunteers, who manually check your R package (especially if it’s the first submission) in addition to all of the automated checks. https://kbroman.org/pkg_primer/pages/cran.html.\nYou can also put your package on Bioconductor, which is reportedly easier: https://bioconductor.org/developers/package-submission/."
  },
  {
    "objectID": "cluster_intro.html",
    "href": "cluster_intro.html",
    "title": "Cluster Computing",
    "section": "",
    "text": "Cluster computing used here refers to the use of remote resources for a computation. This contrasts with local computing, the use of a resource such as a personal or laptop/desktop for a computation. An example could be running a script in a programming language (R, Python, etc.); this could be down with either computing type. What benefits are there in using these remote resources? Doing so can provide the following advantages over local computing:\n\nHave multiple computations running simultaneously\nQueue computations to run automatically\nUse more powerful computational resources (more memory, faster CPU, accessing GPUs, etc.)\n\nwhich can make your computations much more efficient, and sometimes feasible when local resources are not sufficient. This can conceptualized with the following diagram:\n\n\n\nwww.esds.co.in\n\n\nwhere your instructions for the computations are sent to a central remote computer from your local computer, node, which then controls how these are sent out to be completed by the other nodes.\nIn this wiki entry, we will go over the basics of how cluster computing works with the resources available at Penn and how to interface with these resources on a fundamental level. More efficient and powerful ways are covered in the “Advanced Cluster Practices” wiki. Both wiki entries will discuss all of this topics in the context of the takim cluster on PMACS, though there are other clusters which you may have access to which operate slightly differently (e.g. CUBIC). Examples here are down in Windows, but Mac or Linux would be very similar."
  },
  {
    "objectID": "cluster_intro.html#winscp",
    "href": "cluster_intro.html#winscp",
    "title": "Cluster Computing",
    "section": "WinSCP",
    "text": "WinSCP\nAfter downloading and installing WinSCP, you again need to sign-in to the cluster, but this time through WinSCP itself. First select New Session in the upper-left corner, and then select New Site in the newly opened window. Here you specify the details of the cluster, including your log-in info like before. Click Save and then Log-in in this window to log-in to the cluster in WinSCP.\n\n\n\nWinSCP log-in\n\n\nThe window will disappear and you will see two windows; on the left is a file explorer for your local computer and on the right is a file explorer for your cluster directories.\n\n\n\nWinSCP Interface\n\n\nYou can then easily do various tasks such as create new and edit folders, drag and drop folders and files between the two explorers, search for files and folders, etc."
  },
  {
    "objectID": "cluster_intro.html#fetch",
    "href": "cluster_intro.html#fetch",
    "title": "Cluster Computing",
    "section": "Fetch",
    "text": "Fetch\nOn Mac, a good file explorer program is Fetch, which can downloaded at https://fetchsoftworks.com/. It is free to use for educational licenses, so members of Penn and other university can use it for free, otherwise you would have to pay. After installing Fetch and opening the program, a window should pop-up where you specify the server you wish to connect to. If one does not pop-up, you can select File and New connection at the top of the Fetch application. The window should look like this:\n\n\n\nFetch New Connection\n\n\nwhere you then fill-in your cluster information (username, password, etc.). Here is an example of a filled-in log-in to the takim cluster:\n\n\n\nFetch New Connection\n\n\nYou can use the heart icon to save the server address as a favorite to easily access again, as well as the keychain to save passwords. Now that you are logged in, a file directory interface for the cluster will pop up, with the folder displayed being the one specified in the log-in by initial folder, usually you home directory on the server. An example on takim:\n\n\n\nFetch New Connection\n\n\nFrom this window, you can open up folders on the server by clicking on them, navigate the folder structure on the server using the Path icon, and upload/download files to and from the cluster by drag-and-drop as you would usually do with a file explorer interface."
  },
  {
    "objectID": "cluster_intro.html#interactive-jobs",
    "href": "cluster_intro.html#interactive-jobs",
    "title": "Cluster Computing",
    "section": "Interactive Jobs",
    "text": "Interactive Jobs\nAs a motivating example of an interactive job, suppose to do some R programming on a cluster computer. While this might not seem useful since you can program instead on your local computer, interactive jobs can be used to 1) test code on the cluster to make sure it works as expected when you submit the whole set of computations to the cluster and 2) allows to program on-the-fly with a computer likely more powerful then your local computer. You use other programming languages in this way, such as Python, but here we focus on R.\nFirst, we have to start R on the cluster after logging-in. To use, just type in the command R and hit Enter in the command line:\n\n\n\nStarting a R Session\n\n\nThis will open up a session of R on an available cluster node, which we can operate as you usually would with a R terminal. Available nodes are ones which are currently not doing any computations requested by you or other users. The command R will start a default R session, i.e. a version of R denoted as the “default” in the cluster. If you want to use another version of R installed on thr cluster, you would need the corresponding command. If you want to see what software is available and loaded on the cluster (for example, what versions of R are available), use the command module list:\n\n\n\nModule List Command\n\n\nWorking in an interactive R session is identical to running R througn your command line on your local computer or using the R terminal panel in RStudio. To use a GUI-based interface with R interactive R job, see the second set of cluster wikis which discusses Visual Studio Code.\nNow that we have our R session running, we are going to have to deal with 1) working directories and 2) R packages. The default working directory is always your home directory, which you can change using setwd in your R session per-usual. This will let you access files and paths in your R session on the cluster in reference to the working directory with relative path names (or can use absolute path names). R packages are more complicated. By default, a set of R packages has been installed to a central directory on takim which everyone has read access to (can load the packages) but not write access to (cannot install packages). We can use .libPaths() to view this folder’s name and installed. packages() to view which packages these are. It’s a lot of packages, so don’t submit this command only! We look at the first 10 packages by name:\n\n\n\nModule List Command\n\n\nWhat if a package you would like is not installed here? The default library where packages are which users across the cluster can use is not writeable by users. So we need to create our own folder to install packages to and read in when we use R. This is easiest in your File Explorer program (WinSCP or Fetch). For example, we can create a folder in our default home directory on the takim cluster, which only you have read/write access to. For me, the folder’s path is /home/kmdono02/r_packages/ where kmdono02 would be replaced with your username, though you can choose in path you would like. You’ll see below I have a number of installed packages in this folder.\n\n\n\nR Package Directory\n\n\nWhen running R on the cluster, we then have to point to this directory as a libPath so that it knows to look here for a package we may want to load. This can be using the command .libPaths(\"/home/kmdono02/r_packages/\") in your R session/script. We can use the usual library() command in R to load a package installed in the default and added R libPaths\n\n\n\n.LibPaths() Command\n\n\nNow that we have our packages all set up and know how to set our working direcotry, as well as referencing files and folders using absolute and relative path name, we are all set to use the R interactive session on the cluster just like we would if we were running R on our local computer through the R console/terminal. However, we usually don’t rin R through the terminal on our local computer as it is quite cumbersome. Instead we usually use an IDE which is more graphical/user-friendly like RStudio. Luckily we can also use IDE’s to run an R interactive session on the cluster! Two options are RStudio Server and Visual Studio Code (VSCode). VSCode is easier to set up, so we cover how to use it on the cluster in the advanced cluster wiki."
  },
  {
    "objectID": "cluster_intro.html#batch-jobs",
    "href": "cluster_intro.html#batch-jobs",
    "title": "Cluster Computing",
    "section": "Batch Jobs",
    "text": "Batch Jobs\nWhile interactive jobs are very useful, batch jobs in which you send a set of instructions to be carried out remotely by the cluster nodes are a powerful tool and provide the main benefit of cluster computing. These jobs consist of two files: 1) sh file and 2) programming script (R script, Python script). The sh file(s) provide the details about how the job is carried out and then the script provides the computations you want to be done by the nodes. For example, suppose you want to run a linear regression model in R on the cluster? The sh file could provide instructions such as what programming script to run, where to store output on the status of the job from the node, to run a batch job vs array job, etc. In the regression example, suppose we have our R script to run the model in the folder /home/kmdono02/r_scripts/. Then, our sh file could be\ncd /home/kmdono02/r_scripts/ exec R CMD BATCH  --no-save --no-restore run_model.R  ../batch_output/log.out\nThe sh file tells the cluster node two things. First, set the working directory to the folder where the script is stored. Then, run the R script run_model.R in a R session on the node and print the log file to the folder batch_output in the directory /home/kmdono02, with the log file named log.out. The pieces --no-save and --no-restore are just options we can specify to the R CMD BATCH command. Note that these are all just command line commands, equivalent to if we typed these exact lines when we opend the command line in the beginning to interact with the cluster. Putting them in an sh file/script allows us to include a bunch of commands and submit them at once compared to submit each one-by-one, making the process more efficient. These files can also be used to include additional options as hinted at above, but will discuss in the advanced cluster wiki.\nNow that the instructions for our job are all ready, the next step is to actually submit the job to the cluster to be completed. In our sh file-based setup, that means to submit the sh file to the cluster to be completed. For any job, the process is the following:\n\nSubmit job to central node. Job is given a number as an ID in the system, with system specifications provided for the job (max memory, number of CPUs, etc.)\nJob is placed in scheduler, controlled by the central node. On takim, the scheduler is called LSF (though others exist such as SLURM)\nScheduler handles queue of jobs from all users on cluster, depending on order of submission decides which to start\nYour job is next on the queue, central node submits to node to run job\n\nWe go through each process using the linear regression example in R, called run_model.R.\n\nExample\nLet’s consider the following R script:\nlibrary(tidyverse)\n\n# Suspend running commands to pause script to see it in scheduler (since it runs very fast!)\nSys.sleep(30)\n\n# Run model on iris dataset\nmodel &lt;- lm(Petal.Width ~ Petal.Length + Sepal.Width + Sepal.Length, data = iris)\n\n# Store output as txt file\nsink(file = \"/home/kmdono02/cluster_examples/batch/r_output/model_summary.txt\")\nprint(summary(model))\nsink()\nThen, we can specifcy the sh file for instructions on how the cluster should complete this code, called batch.sh:\n#BSUB -o /home/kmdono02/cluster_examples/batch/logs/run_model.out\n#BSUB -e /home/kmdono02/cluster_examples/batch/logs/run_model.err\n\ncd /home/kmdono02/cluster_examples/batch/batch_scripts\nexec R CMD BATCH --no-save --no-restore ../r_scripts/run_model.R  ../output/run_model.out\nThis is similar to file from before, just with some filenames changed and two new lines at the beginning. The lines starting with #BSUB provide the additional instructions to the node for running this job. The options -o and e provide locations to store the output and error files on the LSF-side, which details infomation regarding the node(s) used to complete the job, any errors that occured, etc. To submit this file, we use the following commands in our Command Line after logging into the cluster:\ncd /home/kmdono02/cluster_examples/batch/batch_scripts\nbsub &lt; batch.sh\nThe workhorse function for the job submitting process is bsub, which is where you specify the file detailing the submission. The addition of &lt; enables bsub to “parse” your sh file for arguments labled #BSUB, which will automatically be included as options in the bsub command. We could have also removed those two lines from the sh file and included them directly in the bsub call:\nbsub -o /home/kmdono02/cluster_examples/batch/logs/run_model.out -e /home/kmdono02/cluster_examples/batch/logs/run_model.err sh batch.sh\nthough the first way is what we use from here on in these wikis. What happens after we run bsub? Recall our job enters the queue. To see the status of jobs we have submitted or are currently running, we use thr bjobs command. Let’s call it after running bsub and see the output:\n\n\n\nViewing queue\n\n\nAs you can, it took a second after the job was submitted, but it shortly showed up to the queue under my username with a job ID and starting running immediatey. This meant there was an open node for my job right when I submitted it. In the event there is not such an opening, it will show with the status PENDING. Finally we can look at our folders to view our output. For model_summary.txt, we see the output we asked for R to print\nCall:\nlm(formula = Petal.Width ~ Petal.Length + Sepal.Width + Sepal.Length, \n    data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.60959 -0.10134 -0.01089  0.09825  0.60685 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.24031    0.17837  -1.347     0.18    \nPetal.Length  0.52408    0.02449  21.399  &lt; 2e-16 ***\nSepal.Width   0.22283    0.04894   4.553 1.10e-05 ***\nSepal.Length -0.20727    0.04751  -4.363 2.41e-05 ***\n---\nSignif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1\n\nResidual standard error: 0.192 on 146 degrees of freedom\nMultiple R-squared:  0.9379,    Adjusted R-squared:  0.9366 \nF-statistic: 734.4 on 3 and 146 DF,  p-value: &lt; 2.2e-16\nFrom output/run_model.out, we see everything from the R terminal for this script with a proc.time() call added:\nR version 4.2.2 (2022-10-31) -- \"Innocent and Trusting\"\nCopyright (C) 2022 The R Foundation for Statistical Computing\nPlatform: x86_64-pc-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n  Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n&gt; library(tidyverse)\n Attaching packages  tidyverse 1.3.2 \n ggplot2 3.4.0       purrr   1.0.0 \n tibble  3.1.8       dplyr   1.0.10\n tidyr   1.2.1       stringr 1.5.0 \n readr   2.1.3       forcats 0.5.2 \n Conflicts  tidyverse_conflicts() \n dplyr::filter() masks stats::filter()\n dplyr::lag()    masks stats::lag()\n&gt; \n&gt; # Suspend running commands to pause script to see it in scheduler (since it runs very fast!)\n&gt; Sys.sleep(60)\n&gt; \n&gt; # Run model on iris dataset\n&gt; model &lt;- lm(Petal.Width ~ Petal.Length + Sepal.Width + Sepal.Length, data = iris)\n&gt; \n&gt; # Store output as txt file\n&gt; sink(file = \"/home/kmdono02/cluster_examples/batch/r_output/model_summary.txt\")\n&gt; print(summary(model))\n&gt; sink()\n&gt; \n&gt; proc.time()\n   user  system elapsed \n  3.487   0.451  64.499 \nand finally we have our logs. The error log file is empty since no error occured. File logs/run_model.out shows the cluster-related log:\nSender: LSF System &lt;jszostek@pennsive03&gt;\nSubject: Job 19258593: &lt;#BSUB -o /home/kmdono02/cluster_examples/batch/logs/run_model.out;#BSUB -e /home/kmdono02/cluster_examples/batch/logs/run_model.err; cd /home/kmdono02/cluster_examples/batch/batch_scripts;exec R CMD BATCH --no-save --no-restore ../r_scripts/run_model.R  ../output/run_model.out&gt; in cluster &lt;PMACS-SCC&gt; Done\n\nJob &lt;#BSUB -o /home/kmdono02/cluster_examples/batch/logs/run_model.out;#BSUB -e /home/kmdono02/cluster_examples/batch/logs/run_model.err; cd /home/kmdono02/cluster_examples/batch/batch_scripts;exec R CMD BATCH --no-save --no-restore ../r_scripts/run_model.R  ../output/run_model.out&gt; was submitted from host &lt;takim&gt; by user &lt;kmdono02&gt; in cluster &lt;PMACS-SCC&gt; at Sun Feb 26 13:37:32 2023\nJob was executed on host(s) &lt;pennsive03&gt;, in queue &lt;taki_normal&gt;, as user &lt;kmdono02&gt; in cluster &lt;PMACS-SCC&gt; at Sun Feb 26 13:37:33 2023\n&lt;/home/kmdono02&gt; was used as the home directory.\n&lt;/home/kmdono02/cluster_examples/batch/batch_scripts&gt; was used as the working directory.\nStarted at Sun Feb 26 13:37:33 2023\nTerminated at Sun Feb 26 13:37:39 2023\nResults reported at Sun Feb 26 13:37:39 2023\n\nYour job looked like:\n\n------------------------------------------------------------\n# LSBATCH: User input\n#BSUB -o /home/kmdono02/cluster_examples/batch/logs/run_model.out\n#BSUB -e /home/kmdono02/cluster_examples/batch/logs/run_model.err\n\ncd /home/kmdono02/cluster_examples/batch/batch_scripts\nexec R CMD BATCH --no-save --no-restore ../r_scripts/run_model.R  ../output/run_model.out\n------------------------------------------------------------\n\nSuccessfully completed.\n\nResource usage summary:\n\n    CPU time :                                   3.47 sec.\n    Max Memory :                                 59 MB\n    Average Memory :                             59.00 MB\n    Total Requested Memory :                     -\n    Delta Memory :                               -\n    Max Swap :                                   -\n    Max Processes :                              4\n    Max Threads :                                6\n    Run time :                                   5 sec.\n    Turnaround time :                            7 sec.\n\nThe output (if any) follows:\nwith a handy resource usage summary detailing time of computation, requested (if applicable) and used memory, etc. We have run our first batch job! This format is generalizable for all cases where you want to run a single R script one time. Make sure to specify .libPaths() in your R script if you are using user-installed packages. For running a Python script instead, everything is the same except the use of exec R CMD BATCH. Instead you replace this with python ../py_scripts/run_model.py for example. Requesting an output file from the Python terminal like we did here is a little more complicated as a it must be done in the Python script itself, which we cover in the advanced cluster wiki."
  },
  {
    "objectID": "cluster_intro.html#array-jobs",
    "href": "cluster_intro.html#array-jobs",
    "title": "Cluster Computing",
    "section": "Array Jobs",
    "text": "Array Jobs\nThe last type of job that we cover is an array job. This allows you to submit a sequence of batch jobs simulatenously, which are similar but change in one or more arguments/indices. These jobs are sent to the scheduler at once after submitting one array job, handled like a set of batch jobs of the same size. For example, suppose you have a image processing pipeline to wish to run on an image from 10 subjects. You want to run the same pipeline on each subject, so the code to run on each subject is going to be the same except for reading and writing data based on the subject ID. Instead of writing 10 scripts or writing one script and then submitting 10 separate jobs where we manually specify the subject, can automate it with an array job.\nThe process is the same as with a batch job, in that we will a bsub call, an sh file containing instructions, and then probably some programming language script such as R. The difference is we now have to specify the indices of the array which constitute the array job in the bsub call and sh file. Suppose we want to run our batch job example (linear regression model), but run it on five different sets of data (data_1.csv, …, data_5.csv) where each dataset has outcome y and covariates x1 to x3 (the data sets are simulated from a corresponding linear regression model for illustration). For the bsub call, instead of\ncd /home/kmdono02/cluster_examples/batch/batch_scripts\nbsub &lt; batch.sh\nwe would need to call\ncd /home/kmdono02/cluster_examples/batch/batch_scripts\nbsub &lt; -J \"[1-5]\" batch.sh\nin our cluster terminal. The use of -J \"[1-5]\" tells the scheduler to create an array job with indices 1 to 5. We can use other types of indices, such as -J \"[2,4]\" which will create an array job with only indices 2 and 4. The way these indices relate to what instructions are completed in your sh is that for each job in the array, the corresponding index becomes a Unix environment variable which you can reference inside of your job. This variable has the name LSB_JOBINDEX, which can you use in your R and Python scripts. However, you will also want to have some way to view logs related to each array job to view any errors or output from the cluster which is specific to that job in the array. Recall before we added in #BSUB -e /home/kmdono02/cluster_examples/batch/logs/run_model.err for example to print out an error log for the submitted job. However, this was suboptimal since there is no job ID attached to it. We knew before what it was related to, since we were only running one sh file with the name run_model. Now that we have an array job which contains multiple jobs (5 for example), we need error logs for each one with a name to reference. Luckily this is very easy, and we have two options. First, recall each batch job receives a numeric ID once it has been submitted by the scheduler. This also becomes an environmental variable which we can access, labeled LSB_JOBID. We can also use the variable %J in our sh file and/or bsub commands to refer to the job ID before run. For example, we can replace the previous #BSUB line in our batch.sh file with #BSUB -e /home/kmdono02/cluster_examples/batch/logs/run_model_%J.err to paste in the ID of the given batch job into the file name of that job’s error log and have it be written to our log directory to view. How does this relate to array jobs? Recall an array job is just a sequence of submitted batch job. Thus, each job in our array also gets a job ID to reference, which is contained in LSB_JOBID and %J. However, these IDs are long and not interpretable, what if we just want to refer to the index within our specific array job (job 1, job 2, etc.)? We just use LSB_JOBINDEX and %l respectively instead. We also add in %J or %l to the other #BSUB lines in our sh file, as well as our exec R CMD BATCH call when we specify the Rout filepath.\nLet’s do the regression example with indices 1 through 5. We’ll specify a new sh file, call it batch_array.sh. This sh file will be\n#BSUB -o /home/kmdono02/cluster_examples/batch/logs/run_model_array_%l.out\n#BSUB -e /home/kmdono02/cluster_examples/batch/logs/run_model_array_%l.err\n\ncd /home/kmdono02/cluster_examples/batch/batch_scripts\nexec R CMD BATCH --no-save --no-restore ../r_scripts/run_model_array.R  ../output/run_model_array_$LSB_JOBINDEX.out\nwhere run_model_array.R will be our R script to run the array regression model example. We’ll simulate data within each array job to reflect one of the 5 datasets which we will run the model on, and control them by setting the seed. Thus, run_model_array.R is the following:\nlibrary(tidyverse)\n\n# Suspend running commands to pause script to see it in scheduler (since it runs very fast!)\nSys.sleep(30)\n\n# Generate dataset\nn &lt;- 100\ni &lt;- as.numeric(Sys.getenv('LSB_JOBINDEX'))\nset.seed(i)\nx1 &lt;- rnorm(n); x2 &lt;- rnorm(n); x3 &lt;- rnorm(n); errors &lt;- rnorm(n, mean = 0, sd = 0.25)\ny &lt;- 2*x1+3*x2+4*x3+errors\nsimdata &lt;- data.frame(x1, x2, x3, y)\n\n# Run model\nmodel &lt;- lm(y ~ x1 + x2 + x3, data = simdata)\n\n# Store output as txt file\nsink(file = paste0(\"/home/kmdono02/cluster_examples/batch/r_output/array_model_summary_\", i, \".txt\"))\nprint(summary(model))\nsink()\nWe have simulated data from a linear regression model, of sample size 100, with normally distributed errors with standard deviation 0.25 and 3 covariates of mean 0, with their regression parameters being 2, 3, and 4 respectively. We specify the seed with each dataset as the array index, using Sys.getenv('LSB_JOBINDEX'). This reads-in the environmental variable and then converts to it numeric (is character by default in R). We then fit the regression model and then print the output to our output folder with the index included for reference. We can combine this with the above sh file and below terminal commands to run our array job:\ncd /home/kmdono02/cluster_examples/batch/batch_scripts\nbsub &lt; -J \"[1-5]\" batch_array.sh\nThe queue is now the following after submitting our array jobs:\n\n\n\nViewing queue\n\n\nLet’s do so and look at the outputs. First, the output file for one of the arrays.\nSender: LSF System &lt;jszostek@silver01&gt;\nSubject: Job 19659506[1]: &lt;[1-5]&gt; in cluster &lt;PMACS-SCC&gt; Exited\n\nJob &lt;[1-5]&gt; was submitted from host &lt;takim&gt; by user &lt;kmdono02&gt; in cluster &lt;PMACS-SCC&gt; at Wed Mar 22 17:46:27 2023\nJob was executed on host(s) &lt;silver01&gt;, in queue &lt;taki_normal&gt;, as user &lt;kmdono02&gt; in cluster &lt;PMACS-SCC&gt; at Wed Mar 22 17:46:27 2023\n&lt;/home/kmdono02&gt; was used as the home directory.\n&lt;/home/kmdono02/cluster_examples/batch/batch_scripts&gt; was used as the working directory.\nStarted at Wed Mar 22 17:46:27 2023\nTerminated at Wed Mar 22 17:47:02 2023\nResults reported at Wed Mar 22 17:47:02 2023\n\nYour job looked like:\n\n------------------------------------------------------------\n# LSBATCH: User input\n#BSUB -o /home/kmdono02/cluster_examples/batch/logs/run_model_array_%I.out\n#BSUB -e /home/kmdono02/cluster_examples/batch/logs/run_model_array_%I.err\n\ncd /home/kmdono02/cluster_examples/batch/batch_scripts\nexec R CMD BATCH --no-save --no-restore ../r_scripts/run_model_array.R  ../output/run_model_array_%I.out\n------------------------------------------------------------\n\nExited with exit code 1.\nwhich is just a summary of what was submitted for the entire array job and what the exit code was, just like would happen for a batch job. Then we have 5 resource reports inside the file one for each of the arrays. The other array’s output files are the same: all resource usages for all arrays are stored in each. Then, you could just use #BSUB -o /home/kmdono02/cluster_examples/batch/logs/run_model_array.out to avoid the duplicate files. The R terminal output is stored in ../output/run_model_array_1.out for the first array’s R terminal for example. Finally, we can view the output of the model fit requested by our R script in /home/kmdono02/cluster_examples/batch/r_output/array_model_summary_1 for example, which is as expected. We have now run our first array job!"
  },
  {
    "objectID": "python.html",
    "href": "python.html",
    "title": "Intro to Python",
    "section": "",
    "text": "In this tutorial, we’ll be covering some of the basics of how to use Python to do whatever it is you need to do, including:\n\nPython vs. R\nPackages and evironments\nCompilers\nUsing Python in R\nUsing R in Python\nDataframe manipulation\nVisualization"
  },
  {
    "objectID": "python.html#what-is-a-package",
    "href": "python.html#what-is-a-package",
    "title": "Intro to Python",
    "section": "What is a package?",
    "text": "What is a package?\nIn the same way an R package contains all the functions to complete tasks, Python packages function the same way. There’s two ways of handling this: modules and packages.\nA module is a .py file containing the functions and classes you want to use that you can import into your code. For example, if you had “hello.py” containing the function:\ndef hello(n): \n  for i in range(n): \n    print('hello world') \nYou can import this function into another .py file:\nimport hello\n\nhello.hello(10)\nA package is a group of multiple modules packaged together. Most of the publicly available ones are hosted on PyPI (the Python Package Index), but there are some still in development that are hosted on GitHub. You can install packages from either, as well as develop your own!"
  },
  {
    "objectID": "python.html#installing-packages",
    "href": "python.html#installing-packages",
    "title": "Intro to Python",
    "section": "Installing packages",
    "text": "Installing packages\n\nPyPI\nMost (if not all) commonly used packages are hosted on PyPI. You can directly install packages using the “pip” command. As a general overview of the syntax, you install package (on Windows) using:\npip install packagename\nFor more information on how to install specific versions of a package, uninstalling packages, and more, you can look at the PyPI documentation\n\n\nGitHub\nIf the package can’t be found on PyPI and is instead hosted on GitHub, you can still install it using pip syntax:\npip install --upgrade https://github.com/user/package"
  },
  {
    "objectID": "python.html#environments-conda-pipenvs-etc.",
    "href": "python.html#environments-conda-pipenvs-etc.",
    "title": "Intro to Python",
    "section": "Environments (Conda, Pipenvs, etc.)",
    "text": "Environments (Conda, Pipenvs, etc.)\nThe idea of an environment is important for reproducible programming. An environment can be thought of as the software and packages (with specific version numbers) that you use to code a particular task. For example, using pandas 1.0.2 and numpy 1.4.2 in Python 2.8to write hello.py. This is important for reproducible programming because functions in different package can use different syntax or give slightly different results. Anyone else interested in reproducing your results needs to use the same packages as you to achieve the same results.\nMost environment managers accomplish much the same thing, but some compilers will prefer a particular method (for example, Anaconda compilers function better with conda).\n\nConda\nConda is a generalized environment and package manager that works over many different languages, including Python. Conda can be used to install packages (similar to pip) AND to generate different environments for different codes you want to run or develop. For example, if you wanted to use numpy 1.4.2 for code1.py and numpy 1.2.1 for code2.py, you can create two environments (one for each version) and run each code in its corresponding environment.\nEnvironments can also be shared with others for reproducibility by exporting the .yaml file associated with each environment.\nFor more documentation on the specifics of conda syntax, you can read the conda documentation.\n\n\nPipenv and Venv\nPip and virtual environments are also efficient methods for managing environments and packages.\nVirtualenv or venv (virtual environment) is the lightweight version of pipenv, where you can activate an environment and install packages such that version will be saved in a requirements.txt file for others to use. The only difference between the two is that venv comes with Python3, while virtualenv has to be installed separately.\nPipenv is built on top of pip that allows you to pip install specific packages into an environment, then updates a pipfile with all the packages you’re using. It essentially combines aspects of pip and virtual environments into a single system."
  },
  {
    "objectID": "python.html#spyder",
    "href": "python.html#spyder",
    "title": "Intro to Python",
    "section": "Spyder",
    "text": "Spyder\nSpyder is the IDE that comes with installation of the Anaconda Navigator. For context, the Anaconda Navigator is a GUI set up as the interface for conda environments, Python, and the user. Because of this, Spyder can be run out of conda environments set up through the Anaconda Navigator or the command line.\nCoding in Spyder is most similar to coding R scripts in RStudio, where you can run code in the console and view variables as well as generated graphics. Jupyter notebooks are also integrated with Spyder to enable easy interfacing between the two."
  },
  {
    "objectID": "python.html#pycharm",
    "href": "python.html#pycharm",
    "title": "Intro to Python",
    "section": "PyCharm",
    "text": "PyCharm\nPyCharm is an IDE developed by JetBrains that is similar to Spyder but with a different interface and with more advanced tools for code analysis. The most relevant big difference between PyCharm and Spyder is that the default is to use pipenvs and venvs to set up environments.\nJupyter notebooks can also be run out of and integrated into PyCharm."
  },
  {
    "objectID": "python.html#jupyter-notebook",
    "href": "python.html#jupyter-notebook",
    "title": "Intro to Python",
    "section": "Jupyter Notebook",
    "text": "Jupyter Notebook\nJupyter Notebook is the most interactive of the IDEs, and is most similar to writing code in RMarkdown. Just as in RMarkdown, you can mix chunks of code with markdown text and visualize results in real-time.\nThe one tradeoff is that there is no interface for viewing variables, so debugging becomes more difficult. If you need to more thoroughly analyze the contents of your variables for any reason, it is recommended that you use PyCharm or Spyder."
  },
  {
    "objectID": "python.html#reticulate",
    "href": "python.html#reticulate",
    "title": "Intro to Python",
    "section": "reticulate",
    "text": "reticulate\nThe package “reticulate” is the most popular option for interfacing between R and Python. This interfacing includes importing and calling Python functions as well as passing data between R and Python, and comes with support for Python environments (see next section). There’s two ways of using reticulate:\n1. R Scripting To call Python functions in an R script, you can use the following syntax:\nlibrary(reticulate)\n\n# Import the \"math\" Python package\npy = import(\"math\") \n\n# Print \"math\" implementation of pi\npy$pi\nYou can use the package to essentially run Python in the middle of your R code.\n2. R Markdown\nThis package is already integrated into R Markdown such that if you click the “Add Code Chunk” button in RStudio it’ll automatically embed a Python code chunk in the middle of your RMD code that should look something like this (ignoring the comment signs of course):\n#```{python}\n#print(1)\n#```"
  },
  {
    "objectID": "python.html#rpy2",
    "href": "python.html#rpy2",
    "title": "Intro to Python",
    "section": "rpy2",
    "text": "rpy2\nThe “rpy2” package for Python can be used to interface with R packages, enabling usage of CRAN and other R packages not found in Python. This can be used in both Python scripts and in Jupyter notebooks, and an example is shown below:\n# For importin R datatypes\nimport rpy2.robjects as robjects\n# For importing packages\nfrom rpy2.robjects.packages import importr \n\nbase = importr(\"base\")\nstats = importr(\"stats\")\n\nxs = robjects.FloatVector([1, 2, 3, 4, 5])\nFor more information, you can check out the rpy2 official documentation."
  },
  {
    "objectID": "python.html#numpy",
    "href": "python.html#numpy",
    "title": "Intro to Python",
    "section": "numpy",
    "text": "numpy\nThe numpy package is essentially the package for numeric matrix operations in Python. You’ll find that numpy is included as a dependency in many packages for this reason, and unning matrix operations will often require converting dataframes into numpy matrices. In addition, numpy matrices must all be of the same data type, unlike pandas dataframes, and indexing is only integer-based.\nFor more information on numpy syntax, see the documentation."
  },
  {
    "objectID": "python.html#pandas",
    "href": "python.html#pandas",
    "title": "Intro to Python",
    "section": "pandas",
    "text": "pandas\nThe pandas package is essentially the package for data manipulation. This package provides utilities such as reading/writing data in spreadsheets, merging/grouping dataframes, and more. The pandas DataFrame object is more or less equivalent to the data.frame object in R. Just as in a data.frame object, the pandas DataFrame can contain multiple data types and can be indexed by labels.\nIt is important to note that a pandas Series is distinct from a pandas DataFrame. A Series object will only have one column, while a DataFrame object can have one or more columns. Certain functions will only work for objects of either a Series or DataFrame type, so be aware of which one you’re working with.\nFor more information on pandas syntax, see the documentation."
  },
  {
    "objectID": "python.html#seaborn",
    "href": "python.html#seaborn",
    "title": "Intro to Python",
    "section": "seaborn",
    "text": "seaborn\nThe package seaborn is a higher-level API that is faster, simpler to use, and has a better default aesthetic. However, because it is simpler there is a more limited range of available plot types and it’s harder to manipulate specific components of the plot.\nFor more information on seaborn syntax, see the documentation."
  },
  {
    "objectID": "python.html#matplotlib",
    "href": "python.html#matplotlib",
    "title": "Intro to Python",
    "section": "matplotlib",
    "text": "matplotlib\nThe package matplotlib is a lower-level API that has more available plot types and enables customization of just about every element on the plot. However, because there are more options usage is not as easy as in seaborn, and getting your plot just right can take longer.\nFor more information on matplotlib syntax, see the documentation.\nAlso, fun fact: matplotlib is based on the visualization syntax for MATLAB, so if you’ve used MATLAB before the syntax is very similar!"
  },
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "Contributing to this Wiki",
    "section": "",
    "text": "Thank you for considering making a contribution to the PennSIVE wiki! In this article, you will find a streamlined set of steps that you will need to follow in order to contribute your own article to the PennSIVE wiki website.\nThe requirements to follow this tutorial are:\n\nA GitHub account.\n\nHave Quarto installed (comes with the latest version of Rstudio).\nHave Git installed.\n\nBasic markdown syntax knowledge.\n\nIn essence, making a contribution to this Wiki will involve writing or editing Markdown files and running a few git commands to integrate your changes with the existing wiki. Don’t worry you got this!\n\n\nThe first step to collaborate on any project that is hosted on GitHub (e.g., this wiki) involves “forking”. Forking a GitHub repository (or repo for short) allows you to create your own personal copy of a project under your GitHub account. This forking mechanism allows you to back up changes you make to this wiki’s code without affecting the original repository.\nTo fork this wiki’s source code, follow these steps:\n1. Navigate to https://github.com/pennsive/pennsive.github.io while logged in to GitHub.\n2. Click Fork on the upper right hand corner. \nAfter forking, you can check your personal copy of the wiki’s source code under this URL: https://github.com/&lt;your-username&gt;/pennsive.github.io.\n3. Then, clone your copy of the repository from GitHub onto your local machine. In a Terminal, run:\ngit clone https://github.com/&lt;your-username&gt;/pennsive.github.io\nThis will create a folder called pennsive.github.io at the location where you run this command.\n\n\n\nGitHub allows you to have remote copies of your git repositories. By convention, your personal remote copy of a git repository is called origin. For projects you do not own (e.g., forked projects), there is one more remote copy that you should track: the original repository, which by convention is called upstream.\nTo track the original PennSIVE wiki repository as the upstream repository, you can run:\ncd pennsive.github.io # to get into the project folder\ngit remote add upstream https://github.com/pennsive/pennsive.github.io.git\nTracking the upstream repo will allow you to incorporate incoming changes while you work on your contribution article. Find more details on how this is done in this article [TODO: link to section in git article]\n\n\n\nTo keep your work organized, you will use a branch. A branch is essentially an alternative version of your repository. For more details check the git article [TODO: add link].\ngit checkout -b &lt;yourusername&gt; # if you're making multiple contributions, work on each in a separate branch and give each one a specific name\nTechnically, you could work on the main branch instead of creating another branch for your work. However, branches allow you to incorporate changes easily in the event the main branch of the upstream changes (see rebasing [TODO: add link]). And you will need to be in sync with upstream when contributing your own work. So be safe: use branches.\n\n\n\nOnce you are in your own branch——check that the terminal shows your branch name within parentheses——you’re ready to make changes.\nIn Rstudio, create a Quarto Document (i.e. a file with .qmd extension).\n\n\n\nCreating a Quarto Document\n\n\nThe first line of your document should be a top-level header with ‘#’. This header will be the title of your article, which shows up in the navigation bar of the website. Note that you should not have a YAML section in your document as it is commonly found in Rmarkdown/Quarto documents. For illustration purposes, suppose your document is named article.qmd with a header # Doing cool stuff. Then your newly created document would look like this:\n# Doing cool stuff\nTo add the article to the navigation bar, edit the file _quarto.yml by adding the line - article.qmd under the contents section of the sidebar section. It will look something like this (... represents omitted content):\n# ...\nwebsite:\n# ...\n  sidebar: \n    # ...\n    contents:\n      # ...\n      - article.qmd\n      # ...\nOnce you have added your article to the list of articles in _quarto.yml as described, you can preview the site by running quarto render in the command line OR by pressing the render button in Rstudio. Note: you need the latest version of Rstudio, which comes with Quarto, to use the latter method. \nRendering will create (or update) .html files found under the docs/ directory: these files are the actual content that is viewed in a browser. After rendering, open docs/index.html in a browser and navigate to your article using the sidebar. Voila - A preview of your own version of the PennSIVE wiki! \nOnce you’re familiar with the rendering process, it’s time to generate some content!\nWrite out your contribution article using markdown syntax, version control your work throughout if possible, and follow the next step when you’re ready to submit your work.\n\n\n\nWe will follow a common Git workflow to incorporate our changes to the live wiki. Before commiting your files, make sure you have rendered and previewed the results (there should be changes to the docs/ directory if you run git status)\n\n\nFirst you need to make sure all your work is safe by committing it. To commit [TODO: add link] your work, first add your file to the staging area [TODO: add link].\ngit add article.qmd docs/article.html # replace 'article' by your file name\nThen, add all other files that have been updated, this will include the _quarto.yml file .\ngit add -u # -u adds files that are already tracked in the repo and that have been modified\nOnce all the necessary files are in the staging area, go ahead and commit. Note: make sure that you are in your own branch and NOT on the main branch.\ngit commit -m 'some descriptive message'\nBack up your changes to GitHub before continuing:\ngit push origin &lt;your-branch&gt; \n\n\n\nYou will need to incorporate any changes made to the wiki that may have happened after you forked. To check if changes have been made to the wiki, you need to fetch the metadata from the upstream repository.\ngit checkout main # you should not have uncommitted work before switching to main\ngit fetch upstream main\nOnce the metadata has been fetched, check if your main branch is up to date\ngit status\nIf the resulting message says ‘up to date’ you can skip to (TODO: submitting a pull request). If not follow this process:\ngit pull upstream main\ngit checkout &lt;your-branch&gt;\ngit merge main\nThese three commands will (1) update your main branch to match what is in the original repository you once forked, (2) merge the updated changes from the main branch to &lt;your-branch&gt;. You may need to resolve merge conflicts [TODO: add link] which falls out of the scope of this. However, here’s some additional resources (TODO)\n\n\n\nAfter merging potential updates to the wiki as described above (or if that step was not needed), you can go ahead and push the final version of your work to GitHub.\ngit push origin &lt;your-branch&gt;\nOnce that is successful, navigate to your GitHub account and start a Pull Request. In your copy of the repository, you will see a green button to start a pull request. As an example, here’s the pull request for the article you are currently reading.\n\n\n\nStarting a pull request\n\n\nClicking on the Compare & pull request will take you to a page in the PennSIVE repo where you can describe your edits. Check that the name of your branch appears on the right (see underlined) and the main branch of the PennSIVE repo on the left. Then, include a descriptive summary of your proposed changes/additions. Finally, hit ‘create pull request’.\n.\nOnce you have submitted your pull request, the site maintainer will review your changes and either accept them or ask for more edits. The status of the pull request will be publicly visible on the PennSIVE repo. If more edits are required, make more commits to your branch, push them to your repo, and these changes will appear in your pull request.\n\n\n\nPull Request Submitted\n\n\n\n\n\n\nYou have made a contribution to the PennSIVE wiki and made the world a better place!"
  },
  {
    "objectID": "contributing.html#fork-and-clone-the-wiki-repo",
    "href": "contributing.html#fork-and-clone-the-wiki-repo",
    "title": "Contributing to this Wiki",
    "section": "",
    "text": "The first step to collaborate on any project that is hosted on GitHub (e.g., this wiki) involves “forking”. Forking a GitHub repository (or repo for short) allows you to create your own personal copy of a project under your GitHub account. This forking mechanism allows you to back up changes you make to this wiki’s code without affecting the original repository.\nTo fork this wiki’s source code, follow these steps:\n1. Navigate to https://github.com/pennsive/pennsive.github.io while logged in to GitHub.\n2. Click Fork on the upper right hand corner. \nAfter forking, you can check your personal copy of the wiki’s source code under this URL: https://github.com/&lt;your-username&gt;/pennsive.github.io.\n3. Then, clone your copy of the repository from GitHub onto your local machine. In a Terminal, run:\ngit clone https://github.com/&lt;your-username&gt;/pennsive.github.io\nThis will create a folder called pennsive.github.io at the location where you run this command."
  },
  {
    "objectID": "contributing.html#link-your-local-copy-to-the-upstream-repository",
    "href": "contributing.html#link-your-local-copy-to-the-upstream-repository",
    "title": "Contributing to this Wiki",
    "section": "",
    "text": "GitHub allows you to have remote copies of your git repositories. By convention, your personal remote copy of a git repository is called origin. For projects you do not own (e.g., forked projects), there is one more remote copy that you should track: the original repository, which by convention is called upstream.\nTo track the original PennSIVE wiki repository as the upstream repository, you can run:\ncd pennsive.github.io # to get into the project folder\ngit remote add upstream https://github.com/pennsive/pennsive.github.io.git\nTracking the upstream repo will allow you to incorporate incoming changes while you work on your contribution article. Find more details on how this is done in this article [TODO: link to section in git article]"
  },
  {
    "objectID": "contributing.html#create-your-own-branch",
    "href": "contributing.html#create-your-own-branch",
    "title": "Contributing to this Wiki",
    "section": "",
    "text": "To keep your work organized, you will use a branch. A branch is essentially an alternative version of your repository. For more details check the git article [TODO: add link].\ngit checkout -b &lt;yourusername&gt; # if you're making multiple contributions, work on each in a separate branch and give each one a specific name\nTechnically, you could work on the main branch instead of creating another branch for your work. However, branches allow you to incorporate changes easily in the event the main branch of the upstream changes (see rebasing [TODO: add link]). And you will need to be in sync with upstream when contributing your own work. So be safe: use branches."
  },
  {
    "objectID": "contributing.html#work-on-your-contribution-article",
    "href": "contributing.html#work-on-your-contribution-article",
    "title": "Contributing to this Wiki",
    "section": "",
    "text": "Once you are in your own branch——check that the terminal shows your branch name within parentheses——you’re ready to make changes.\nIn Rstudio, create a Quarto Document (i.e. a file with .qmd extension).\n\n\n\nCreating a Quarto Document\n\n\nThe first line of your document should be a top-level header with ‘#’. This header will be the title of your article, which shows up in the navigation bar of the website. Note that you should not have a YAML section in your document as it is commonly found in Rmarkdown/Quarto documents. For illustration purposes, suppose your document is named article.qmd with a header # Doing cool stuff. Then your newly created document would look like this:\n# Doing cool stuff\nTo add the article to the navigation bar, edit the file _quarto.yml by adding the line - article.qmd under the contents section of the sidebar section. It will look something like this (... represents omitted content):\n# ...\nwebsite:\n# ...\n  sidebar: \n    # ...\n    contents:\n      # ...\n      - article.qmd\n      # ...\nOnce you have added your article to the list of articles in _quarto.yml as described, you can preview the site by running quarto render in the command line OR by pressing the render button in Rstudio. Note: you need the latest version of Rstudio, which comes with Quarto, to use the latter method. \nRendering will create (or update) .html files found under the docs/ directory: these files are the actual content that is viewed in a browser. After rendering, open docs/index.html in a browser and navigate to your article using the sidebar. Voila - A preview of your own version of the PennSIVE wiki! \nOnce you’re familiar with the rendering process, it’s time to generate some content!\nWrite out your contribution article using markdown syntax, version control your work throughout if possible, and follow the next step when you’re ready to submit your work."
  },
  {
    "objectID": "contributing.html#integrate-your-work-with-the-pennsive-wiki",
    "href": "contributing.html#integrate-your-work-with-the-pennsive-wiki",
    "title": "Contributing to this Wiki",
    "section": "",
    "text": "We will follow a common Git workflow to incorporate our changes to the live wiki. Before commiting your files, make sure you have rendered and previewed the results (there should be changes to the docs/ directory if you run git status)\n\n\nFirst you need to make sure all your work is safe by committing it. To commit [TODO: add link] your work, first add your file to the staging area [TODO: add link].\ngit add article.qmd docs/article.html # replace 'article' by your file name\nThen, add all other files that have been updated, this will include the _quarto.yml file .\ngit add -u # -u adds files that are already tracked in the repo and that have been modified\nOnce all the necessary files are in the staging area, go ahead and commit. Note: make sure that you are in your own branch and NOT on the main branch.\ngit commit -m 'some descriptive message'\nBack up your changes to GitHub before continuing:\ngit push origin &lt;your-branch&gt; \n\n\n\nYou will need to incorporate any changes made to the wiki that may have happened after you forked. To check if changes have been made to the wiki, you need to fetch the metadata from the upstream repository.\ngit checkout main # you should not have uncommitted work before switching to main\ngit fetch upstream main\nOnce the metadata has been fetched, check if your main branch is up to date\ngit status\nIf the resulting message says ‘up to date’ you can skip to (TODO: submitting a pull request). If not follow this process:\ngit pull upstream main\ngit checkout &lt;your-branch&gt;\ngit merge main\nThese three commands will (1) update your main branch to match what is in the original repository you once forked, (2) merge the updated changes from the main branch to &lt;your-branch&gt;. You may need to resolve merge conflicts [TODO: add link] which falls out of the scope of this. However, here’s some additional resources (TODO)\n\n\n\nAfter merging potential updates to the wiki as described above (or if that step was not needed), you can go ahead and push the final version of your work to GitHub.\ngit push origin &lt;your-branch&gt;\nOnce that is successful, navigate to your GitHub account and start a Pull Request. In your copy of the repository, you will see a green button to start a pull request. As an example, here’s the pull request for the article you are currently reading.\n\n\n\nStarting a pull request\n\n\nClicking on the Compare & pull request will take you to a page in the PennSIVE repo where you can describe your edits. Check that the name of your branch appears on the right (see underlined) and the main branch of the PennSIVE repo on the left. Then, include a descriptive summary of your proposed changes/additions. Finally, hit ‘create pull request’.\n.\nOnce you have submitted your pull request, the site maintainer will review your changes and either accept them or ask for more edits. The status of the pull request will be publicly visible on the PennSIVE repo. If more edits are required, make more commits to your branch, push them to your repo, and these changes will appear in your pull request.\n\n\n\nPull Request Submitted"
  },
  {
    "objectID": "heudiconv.html",
    "href": "heudiconv.html",
    "title": "Dicom Conversion",
    "section": "",
    "text": "When labs organize datasets in different ways time is often wasted rewriting scripts to expect a particular structure. BIDS solves this problem by standardizing the organization of neuroimaging data. BIDS is nothing more than a prescribed directory structure and file naming convention, but it allows labs to more easily share datasets and processing pipelines, interact with the dataset programmatically via pybids, and run all the most popular neuroimaging tools (“BIDS Apps”) in a consistent way.\nAlthough it’s possible to manually organize NIFTIs into the BIDS structure, doing so is error-prone and inefficient. Manual organization may be the only option if dicoms aren’t available, but otherwise you can use heudiconv to automatically format the produced NIFTIs according to BIDS.\nUnlike dcm2niix which can simply be fed a directory of dicoms, heudiconv requires a bit more setup. First, your dicoms be organized either by StudyUID or accession_number. Then, you need to run heudiconv with the -c none -f convertall options to generate TSVs with dicom metadata needed to write a heuristic.\nFor example,\nheudiconv \\\n    -d \"/path/to/data/{subject}/{session}/*.dcm\" \\\n    -o /path/to/output \\\n    -f convertall \\\n    -s 001 -ss abc -c none -b -g accession_number\nThere will now be a hidden .heudiconv directory with dicom header TSVs. Once you’ve determined which dicoms go where, write a heuristic then use the -c dcm2niix option to tell heudiconv to do the actual dcm2niix conversion.\nheudiconv \\\n    -d \"/path/to/data/{subject}/{session}/*.dcm\" \\\n    -o /path/to/output \\\n    -f /path/to/your_heuristic.py \\\n    -s 001 -ss abc -c dcm2niix -b -g accession_number"
  },
  {
    "objectID": "heudiconv.html#bids",
    "href": "heudiconv.html#bids",
    "title": "Dicom Conversion",
    "section": "",
    "text": "When labs organize datasets in different ways time is often wasted rewriting scripts to expect a particular structure. BIDS solves this problem by standardizing the organization of neuroimaging data. BIDS is nothing more than a prescribed directory structure and file naming convention, but it allows labs to more easily share datasets and processing pipelines, interact with the dataset programmatically via pybids, and run all the most popular neuroimaging tools (“BIDS Apps”) in a consistent way.\nAlthough it’s possible to manually organize NIFTIs into the BIDS structure, doing so is error-prone and inefficient. Manual organization may be the only option if dicoms aren’t available, but otherwise you can use heudiconv to automatically format the produced NIFTIs according to BIDS.\nUnlike dcm2niix which can simply be fed a directory of dicoms, heudiconv requires a bit more setup. First, your dicoms be organized either by StudyUID or accession_number. Then, you need to run heudiconv with the -c none -f convertall options to generate TSVs with dicom metadata needed to write a heuristic.\nFor example,\nheudiconv \\\n    -d \"/path/to/data/{subject}/{session}/*.dcm\" \\\n    -o /path/to/output \\\n    -f convertall \\\n    -s 001 -ss abc -c none -b -g accession_number\nThere will now be a hidden .heudiconv directory with dicom header TSVs. Once you’ve determined which dicoms go where, write a heuristic then use the -c dcm2niix option to tell heudiconv to do the actual dcm2niix conversion.\nheudiconv \\\n    -d \"/path/to/data/{subject}/{session}/*.dcm\" \\\n    -o /path/to/output \\\n    -f /path/to/your_heuristic.py \\\n    -s 001 -ss abc -c dcm2niix -b -g accession_number"
  },
  {
    "objectID": "pipelines.html",
    "href": "pipelines.html",
    "title": "PennSIVE pipelines",
    "section": "",
    "text": "Generally we use R and develop R packages, but many of our tools have python and command-line interfaces as well.\nIf you’re building a package that uses a PennSIVE tool, or need to fine-tune parameters not exposed by the wrappers, you’re best off using the R package. If you’re using python you can use a rpy2-based wrapper and be able to keep parameters that represent NIFTI images in memory. If you’re building a neuroimaging pipeline in python with Nipype, we have a fork with our tools called PennSIVEpype. Otherwise, it’s best to use the command-line wrappers.\n\n\nMIMoSA automates multiple sclerosis (MS) lesion segmentation with just a T1w and FLAIR required.\n\ngithub.com/avalcarcel9/mimosa\n\nAvailable as a BIDS app, on docker hub. For example:\n# singularity pull docker://pennsive/mimosa\nsingularity run --cleanenv --bind ${PWD} --bind ${TMPDIR} \\\n        /path/to/image \\\n        inputs/data \\\n        outputs \\\n        participant \\\n        --participant_label $sub_num \\\n        --strip mass \\\n        --n4 \\\n        --register \\\n        --whitestripe \\\n        --thresh 0.25 \\\n        --debug \\\n        --skip_bids_validator\nTo use as a datalad bootstrap:\ncurl -LO https://raw.githubusercontent.com/PennLINC/TheWay/main/scripts/pmacs/bootstrap-mimosa.sh\nbash bootstrap-mimosa.sh --bids-input ria+file:///path/to/bids  --container-ds /path/or/uri/to/containers\ncd mimosa/analysis\nbash code/bsub_calls.sh\n# when jobs complete\nbash code/merge_outputs.sh\nIf your data are not in BIDS it’s possible to run the CLI of mimosa directly on a T1/FLAIR pair by using docker run --entrypoint=\"\" instead docker run or singularity exec instead of singularity run and then calling /run.R directly. For example:\nsingularity exec --cleanenv --bind ${PWD} --bind ${TMPDIR} \\\n        /path/to/image /run.R \\\n        -i /path/to/folder/with/niftis \\\n        -o /path/to/outdir \\\n        -f flair.nii.gz \\\n        -t t1.nii.gz \\\n        -s mass \\\n        --n4 \\\n        --whitestripe\nAs a nipype interface:\nfrom nipype.interfaces.mimosa import MIMoSA\nmimosa = MIMoSA()\nmimosa.inputs.t1 = \"sub-01_mimosa-0.3.0/mimosa/t1_ws.nii.gz\"\nmimosa.inputs.flair = \"sub-01_mimosa-0.3.0/mimosa/flair_ws.nii.gz\"\nmimosa.inputs.tissue = True\nmimosa.inputs.verbose = True\nmimosa.inputs.cores = 1\nmimosa.run()\n\n\n\nThe central vein sign (CVS) uses a lesion probability map (from MIMoSA) and a vessellness filtering process to find veins running through lesions, biomarker of MS.\n\ngithub.com/jdwor/cvs\n\nAvailable as a BIDS app, on docker hub. For example:\nsingularity run --cleanenv --bind ${PWD} --bind ${TMPDIR} \\\n        /path/to/image \\\n        inputs/data \\\n        outputs \\\n        participant \\\n        --participant_label $sub_num \\\n        --skullstrip \\\n        --n4 \\\n        --thresh 0.25 \\\n        --skip_bids_validator\nAs a nipype interface:\nfrom nipype.interfaces.cvs import CVS\ncvs = CVS()\ncvs.inputs.t1 = \"t1.nii.gz\"\ncvs.inputs.flair = \"flair.nii.gz\"\ncvs.inputs.epi = \"epi.nii.gz\"\ncvs.inputs.mimosa_prob_map = \"mimosa_prob_map.nii.gz\"\ncvs.inputs.mimosa_bin_map = \"mimosa_bin_map.nii.gz\"\ncvs.inputs.candidate_lesions = \"candidate_lesions.npy\"\ncvs.inputs.cvs_prob_map = \"prob_map.npy\"\ncvs.inputs.biomarker = \"biomarker.npy\"\ncvs.inputs.parallel = False\ncvs.inputs.skullstripped = True\ncvs.inputs.biascorrected = True\ncvs.inputs.c3d = True\ncvs.inputs.cores = 1\ncvs.run()\n\n\n\nThe presence of paramagnetic rims around lesions (PRL) is another MS biomarker.\n\ngithub.com/carolynlou/prlr\n\nAvailable as a BIDS app, on docker hub. For example:\nsingularity run --cleanenv --bind ${PWD} --bind ${TMPDIR} \\\n        /path/to/image \\\n        inputs/data \\\n        outputs \\\n        participant \\\n        --participant_label $sub_num\nAs a nipype interface:\nfrom nipype.interfaces.prls import PRL\nprl = PRL()\nprl.inputs.prob_map = \"prob_map.nii.gz\"\nprl.inputs.lesion_map = \"lesion_map.nii.gz\"\nprl.inputs.phase = \"phase.nii.gz\"\nprl.inputs.disc = True\nprl.run()\n\n\n\nFrom the lesiontools package, the lesion center detection method helps count distinct lesions.\n\ngithub.com/jdwor/lesiontools\n\nAvailable as a BIDS app, on docker hub. For example:\nsingularity run --cleanenv --bind ${PWD} --bind ${TMPDIR} \\\n        /path/to/image \\\n        inputs/data \\\n        outputs \\\n        participant \\\n        --participant_label $sub_num \\\n        --strip mass \\\n        --n4 \\\n        --min-center-size 10 \\\n        --gmm \\\n        --skip_bids_validator\nAs a nipype interface:\nfrom nipype.interfaces.lesionclusters import LesionClusters\nclusters = LesionClusters()\nclusters.inputs.prob_map = \"prob_map.nii.gz\"\nclusters.inputs.bin_map = \"bin_map.nii.gz\"\nclusters.inputs.centers = \"centers.nii.gz\"\nclusters.inputs.nnmap = \"nnmap.nii.gz\"\nclusters.inputs.clusmap = \"clusmap.nii.gz\"\nclusters.inputs.gmmmap = \"gmmmap.nii.gz\"\nclusters.inputs.gmm = True\nclusters.inputs.parallel = True\nclusters.inputs.cores = 4\nclusters.inputs.smooth = 1.2\nclusters.inputs.min_center_size = 10\nclusters.run()"
  },
  {
    "objectID": "pipelines.html#mimosa",
    "href": "pipelines.html#mimosa",
    "title": "PennSIVE pipelines",
    "section": "",
    "text": "MIMoSA automates multiple sclerosis (MS) lesion segmentation with just a T1w and FLAIR required.\n\ngithub.com/avalcarcel9/mimosa\n\nAvailable as a BIDS app, on docker hub. For example:\n# singularity pull docker://pennsive/mimosa\nsingularity run --cleanenv --bind ${PWD} --bind ${TMPDIR} \\\n        /path/to/image \\\n        inputs/data \\\n        outputs \\\n        participant \\\n        --participant_label $sub_num \\\n        --strip mass \\\n        --n4 \\\n        --register \\\n        --whitestripe \\\n        --thresh 0.25 \\\n        --debug \\\n        --skip_bids_validator\nTo use as a datalad bootstrap:\ncurl -LO https://raw.githubusercontent.com/PennLINC/TheWay/main/scripts/pmacs/bootstrap-mimosa.sh\nbash bootstrap-mimosa.sh --bids-input ria+file:///path/to/bids  --container-ds /path/or/uri/to/containers\ncd mimosa/analysis\nbash code/bsub_calls.sh\n# when jobs complete\nbash code/merge_outputs.sh\nIf your data are not in BIDS it’s possible to run the CLI of mimosa directly on a T1/FLAIR pair by using docker run --entrypoint=\"\" instead docker run or singularity exec instead of singularity run and then calling /run.R directly. For example:\nsingularity exec --cleanenv --bind ${PWD} --bind ${TMPDIR} \\\n        /path/to/image /run.R \\\n        -i /path/to/folder/with/niftis \\\n        -o /path/to/outdir \\\n        -f flair.nii.gz \\\n        -t t1.nii.gz \\\n        -s mass \\\n        --n4 \\\n        --whitestripe\nAs a nipype interface:\nfrom nipype.interfaces.mimosa import MIMoSA\nmimosa = MIMoSA()\nmimosa.inputs.t1 = \"sub-01_mimosa-0.3.0/mimosa/t1_ws.nii.gz\"\nmimosa.inputs.flair = \"sub-01_mimosa-0.3.0/mimosa/flair_ws.nii.gz\"\nmimosa.inputs.tissue = True\nmimosa.inputs.verbose = True\nmimosa.inputs.cores = 1\nmimosa.run()"
  },
  {
    "objectID": "pipelines.html#cvs",
    "href": "pipelines.html#cvs",
    "title": "PennSIVE pipelines",
    "section": "",
    "text": "The central vein sign (CVS) uses a lesion probability map (from MIMoSA) and a vessellness filtering process to find veins running through lesions, biomarker of MS.\n\ngithub.com/jdwor/cvs\n\nAvailable as a BIDS app, on docker hub. For example:\nsingularity run --cleanenv --bind ${PWD} --bind ${TMPDIR} \\\n        /path/to/image \\\n        inputs/data \\\n        outputs \\\n        participant \\\n        --participant_label $sub_num \\\n        --skullstrip \\\n        --n4 \\\n        --thresh 0.25 \\\n        --skip_bids_validator\nAs a nipype interface:\nfrom nipype.interfaces.cvs import CVS\ncvs = CVS()\ncvs.inputs.t1 = \"t1.nii.gz\"\ncvs.inputs.flair = \"flair.nii.gz\"\ncvs.inputs.epi = \"epi.nii.gz\"\ncvs.inputs.mimosa_prob_map = \"mimosa_prob_map.nii.gz\"\ncvs.inputs.mimosa_bin_map = \"mimosa_bin_map.nii.gz\"\ncvs.inputs.candidate_lesions = \"candidate_lesions.npy\"\ncvs.inputs.cvs_prob_map = \"prob_map.npy\"\ncvs.inputs.biomarker = \"biomarker.npy\"\ncvs.inputs.parallel = False\ncvs.inputs.skullstripped = True\ncvs.inputs.biascorrected = True\ncvs.inputs.c3d = True\ncvs.inputs.cores = 1\ncvs.run()"
  },
  {
    "objectID": "pipelines.html#prl",
    "href": "pipelines.html#prl",
    "title": "PennSIVE pipelines",
    "section": "",
    "text": "The presence of paramagnetic rims around lesions (PRL) is another MS biomarker.\n\ngithub.com/carolynlou/prlr\n\nAvailable as a BIDS app, on docker hub. For example:\nsingularity run --cleanenv --bind ${PWD} --bind ${TMPDIR} \\\n        /path/to/image \\\n        inputs/data \\\n        outputs \\\n        participant \\\n        --participant_label $sub_num\nAs a nipype interface:\nfrom nipype.interfaces.prls import PRL\nprl = PRL()\nprl.inputs.prob_map = \"prob_map.nii.gz\"\nprl.inputs.lesion_map = \"lesion_map.nii.gz\"\nprl.inputs.phase = \"phase.nii.gz\"\nprl.inputs.disc = True\nprl.run()"
  },
  {
    "objectID": "pipelines.html#center-detection",
    "href": "pipelines.html#center-detection",
    "title": "PennSIVE pipelines",
    "section": "",
    "text": "From the lesiontools package, the lesion center detection method helps count distinct lesions.\n\ngithub.com/jdwor/lesiontools\n\nAvailable as a BIDS app, on docker hub. For example:\nsingularity run --cleanenv --bind ${PWD} --bind ${TMPDIR} \\\n        /path/to/image \\\n        inputs/data \\\n        outputs \\\n        participant \\\n        --participant_label $sub_num \\\n        --strip mass \\\n        --n4 \\\n        --min-center-size 10 \\\n        --gmm \\\n        --skip_bids_validator\nAs a nipype interface:\nfrom nipype.interfaces.lesionclusters import LesionClusters\nclusters = LesionClusters()\nclusters.inputs.prob_map = \"prob_map.nii.gz\"\nclusters.inputs.bin_map = \"bin_map.nii.gz\"\nclusters.inputs.centers = \"centers.nii.gz\"\nclusters.inputs.nnmap = \"nnmap.nii.gz\"\nclusters.inputs.clusmap = \"clusmap.nii.gz\"\nclusters.inputs.gmmmap = \"gmmmap.nii.gz\"\nclusters.inputs.gmm = True\nclusters.inputs.parallel = True\nclusters.inputs.cores = 4\nclusters.inputs.smooth = 1.2\nclusters.inputs.min_center_size = 10\nclusters.run()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the Penn Statistics in Imaging and Visualization Endeavor (PennSIVE) Center",
    "section": "",
    "text": "Welcome to the Penn Statistics in Imaging and Visualization Endeavor (PennSIVE) Center\nThis is the internal website for PennSIVE tutorials and best practices.\nIf you just got cluster access, first setup your SSH keys:\nssh-keygen # if you've never done this or aren't sure; you can accept all the defaults, and don't need a passphrase\nssh-copy-id user@cluster\nThen download VS code, docker, and read through the environment setup guide.\nWhen setting up a project meant for publication, read through the project setup guide which explains how to version control code with git, track data provenance with datalad, and keep data organized according to the BIDS standard.\nFor more specific python neuroimaging tutorials, see the many nipype examples. For more specific R neuroimaging tutorials, see neuroconductor.\nThe CBICA wiki (UPHS VPN required) is a good resource for CUBIC-specific tutorials, and Ali’s blog offers lots of useful PMACS-specific information.\nAlso see the PennLINC internal website which offers many useful tutorials and tips."
  },
  {
    "objectID": "workflows.html",
    "href": "workflows.html",
    "title": "Nipype workflows",
    "section": "",
    "text": "Nipype workflows\nMany neuroimaging tools have idiosyncratic CLIs that are clumsy to use from programming languages like python and R. Nipype provides a unified interface that facilitates the design of workflows within and between packages, lowering the learning curve necessary to use a new package.\nNipype interfaces consist of an input and output specification class, _run_interface method, and _list_outputs. To validate inputs are the requested type and outputs get created correctly, nipype uses a package called Traits to automate much of the process. When the .run() method is called on an instance of your interface, the _run_interface method is called, and _list_outputs is used to list the files matched against the output specification. For example, an interface that simply moves a file might look like\nclass MoveResultFileInputSpec(BaseInterfaceInputSpec):\n    in_file = File(exists=True, desc='input file to be renamed', mandatory=True)\n    output_name = traits.String(desc='output name string')\n\n\nclass MoveResultFileOutputSpec(TraitedSpec):\n    out_file = File(desc='path of moved file')\n\n\nclass MoveResultFile(BaseInterface):\n    input_spec = MoveResultFileInputSpec\n    output_spec = MoveResultFileOutputSpec\n\n    def _run_interface(self, runtime):\n        shutil.copyfile(self.inputs.in_file, self.inputs.output_name)\n        return runtime\n\n    def _list_outputs(self):\n        outputs = self._outputs().get()\n        outputs['out_file'] = self.inputs.output_name\n        return outputs\nA slightly more complex example, which thresholds an input image with nibabel might look like:\nfrom nipype.interfaces.base import BaseInterface, \\\n    BaseInterfaceInputSpec, traits, File, TraitedSpec\nfrom nipype.utils.filemanip import split_filename\n\nimport nibabel as nb\nimport numpy as np\nimport os\n\nclass SimpleThresholdInputSpec(BaseInterfaceInputSpec):\n    volume = File(exists=True, desc='volume to be thresholded', mandatory=True)\n    threshold = traits.Float(desc='everything below this value will be set to zero',\n                             mandatory=True)\n\n\nclass SimpleThresholdOutputSpec(TraitedSpec):\n    thresholded_volume = File(exists=True, desc=\"thresholded volume\")\n\n\nclass SimpleThreshold(BaseInterface):\n    input_spec = SimpleThresholdInputSpec\n    output_spec = SimpleThresholdOutputSpec\n\n    def _run_interface(self, runtime):\n        fname = self.inputs.volume\n        img = nb.load(fname)\n        data = np.array(img.get_data())\n\n        active_map = data &gt; self.inputs.threshold\n\n        thresholded_map = np.zeros(data.shape)\n        thresholded_map[active_map] = data[active_map]\n\n        new_img = nb.Nifti1Image(thresholded_map, img.affine, img.header)\n        _, base, _ = split_filename(fname)\n        nb.save(new_img, base + '_thresholded.nii')\n\n        return runtime\n\n    def _list_outputs(self):\n        outputs = self._outputs().get()\n        fname = self.inputs.volume\n        _, base, _ = split_filename(fname)\n        outputs[\"thresholded_volume\"] = os.path.abspath(base + '_thresholded.nii')\n        return outputs\nWhen writing workflows, most of the time a tool will already have an interface, see the nipype interfaces index for a complete list. A workflow will typically begin by parsing command line arguments\nparser = argparse.ArgumentParser()\nparser.add_argument('-i', '--input', type=str, default='T1W.nii.gz')\nparser.add_argument('-o', '--output', type=str, default='/tmp/tmp')\nargs = parser.parse_args()\nsetup a workflow and interface node:\nwf = Workflow('threshold')\n\nthresh_phase = Node(SimpleThreshold(), 'thresh_phase')\nthresh_phase.inputs.volume = args.input\nthresh_phase.inputs.threshold = 0.5\nthen another node, connected to the first\nmove_phase = Node(MoveResultFile(), 'move_phase')\nmove_phase.inputs.output_name = args.output\nwf.connect([(thresh_phase, move_phase, [('out_file', 'input_image')])])"
  },
  {
    "objectID": "environments.html",
    "href": "environments.html",
    "title": "Environment setup",
    "section": "",
    "text": "Neuroimaging software can be difficult to install and have complex dependencies. If any tool has a different version or is configured differently, results can become impossible to reproduce. So we use containers which are like lightweight VMs that turn your environment into code so it can be shared, version controlled, and reproduced on any machine that supports a container runtime. On platforms where you have root access (like your computer), docker is the most convenient container runtime, but Singularity is a good alternative for rootless containers in cluster environments.\nThere are some general purpose container images you can pull to do experimenting, but new projects with public results should create a project-specific container by writing a Dockerfile. NeuroDocker automates much of this process,\ndocker run --rm repronim/neurodocker:0.7.0 generate docker \\\n    --pkg-manager apt \\\n    --base debian:buster \\\n    --ants version=2.3.1 \\\n    --fsl version=6.0.3\nwhich will write a Dockerfile to stdout (you can pipe directly into docker build) but any software they don’t support has to be installed in that or another Dockerfile.\n\n\n\n\n\n\nRunning a container with docker follows the format: docker run [options for docker] image_name [payload process arguments]. For example,\ndocker run -it -v $PWD:/data -w /data --rm pennsive/neuror:4.0 R -e \"list.files()\"\nwill run the pennsive/neuror container interactively (-it), setting up a bind mount (-v), setting the working directory for the payload process to that bind mount (-w), and remove the container after completion (--rm). Download docker desktop for mac.\n\n\n\nDownload vs code for mac, then install all the essential extensions with\nwhich code && for extension in $(curl -s https://raw.githubusercontent.com/PennSIVE/pennsive.github.io/main/vs-code-extension-list.txt); do\ncode --install-extension $extension\ndone || echo \"in the vscode command pallet (command + shift + p) search for \\\"Install code command in 'PATH'\\\"\"\n\n\n\nClone the lab’s bash/zsh startup scripts, which creates some convenient aliases\ncd ~/repos\ngit clone https://github.com/PennSIVE/bash.git\ncd bash\ncp .env.sample .env\nvim .env # edit as necessary\n# open your ~/.bashrc or ~/.zshrc and add:\nif [ -f $HOME/repos/bash/.bashrc ]; then\n    source $HOME/repos/bash/.bashrc\nfi\nTry creating a SSH tunnel for HTTP traffic (takimhttp or cbicahttp) then starting R studio with rstudio (which should then be available at http://localhost).\n\n\n\nhomebrew, starship, ITK-SNAP\n\n\n\n\n\nThe PMACS cluster is generally more amicable to interactive work while CUBIC is generally better for large batch and GPU jobs. Singularity is installed on both clusters, but on PMACS it’s installed as an environment module so you need to run module load DEV/singularity first.\nSingularity commands have different semantics than docker commands, but also follow the general format singularity run [options for singularity] image_name [payload process arguments]. The singularity equivalent of -it is singularity shell (instead of singularity run), instead of -v it’s -B, instead of -w it’s --pwd.\nBeyond semantics there are a couple major difference between docker and singularity. First, singularity by default does less to isolate the container than docker does, so you’ll likely always need the --cleanenv or --containall flags to prevent host environment variables from overwriting those in the container. Moreover, it’s good practice to make explicit the environment variables your program uses. Second, all paths in a singularity container are read-only unless that path is bind mounted. Third, singularity bind mounts your current working directory and home directory by default. /scratch, the default $TMDIR on the clusters, is not bind mounted by default so it’s you’ll likely always need to include -B /scratch in your singularity command.\n\n\nPMACS uses the LSF platform, which uses bsub to submit jobs. To submit an interactive job, run bsub -Is -q \"$QUEUE\"_interactive 'bash';. To run a non-interactive job, run bsub -o /path/to/stdout -e /path/to/stderr ./my_job.sh\n\n\n\nCUBIC uses the SGE platform, which uses qsub to submit jobs. There are no interactive compute nodes, but the login nodes are fairly beefy. To run a non-interactive job, run qsub -o /path/to/stdout -e /path/to/stderr -b y -cwd -l h_vmem=16G -pe threaded 4-8 ./my_job.sh. -b y tells SGE you’re running a binary executable, -cwd makes the directory you issue the command from the working directory for the job, -l h_vmem=16G sets memory (default is only 4G!), and -pe threaded 4-8 gives your job anywhere from 4-8 CPU cores depending on what the scheduler decides."
  },
  {
    "objectID": "environments.html#setting-up-your-environment",
    "href": "environments.html#setting-up-your-environment",
    "title": "Environment setup",
    "section": "",
    "text": "Neuroimaging software can be difficult to install and have complex dependencies. If any tool has a different version or is configured differently, results can become impossible to reproduce. So we use containers which are like lightweight VMs that turn your environment into code so it can be shared, version controlled, and reproduced on any machine that supports a container runtime. On platforms where you have root access (like your computer), docker is the most convenient container runtime, but Singularity is a good alternative for rootless containers in cluster environments.\nThere are some general purpose container images you can pull to do experimenting, but new projects with public results should create a project-specific container by writing a Dockerfile. NeuroDocker automates much of this process,\ndocker run --rm repronim/neurodocker:0.7.0 generate docker \\\n    --pkg-manager apt \\\n    --base debian:buster \\\n    --ants version=2.3.1 \\\n    --fsl version=6.0.3\nwhich will write a Dockerfile to stdout (you can pipe directly into docker build) but any software they don’t support has to be installed in that or another Dockerfile."
  },
  {
    "objectID": "environments.html#local-development",
    "href": "environments.html#local-development",
    "title": "Environment setup",
    "section": "",
    "text": "Running a container with docker follows the format: docker run [options for docker] image_name [payload process arguments]. For example,\ndocker run -it -v $PWD:/data -w /data --rm pennsive/neuror:4.0 R -e \"list.files()\"\nwill run the pennsive/neuror container interactively (-it), setting up a bind mount (-v), setting the working directory for the payload process to that bind mount (-w), and remove the container after completion (--rm). Download docker desktop for mac.\n\n\n\nDownload vs code for mac, then install all the essential extensions with\nwhich code && for extension in $(curl -s https://raw.githubusercontent.com/PennSIVE/pennsive.github.io/main/vs-code-extension-list.txt); do\ncode --install-extension $extension\ndone || echo \"in the vscode command pallet (command + shift + p) search for \\\"Install code command in 'PATH'\\\"\"\n\n\n\nClone the lab’s bash/zsh startup scripts, which creates some convenient aliases\ncd ~/repos\ngit clone https://github.com/PennSIVE/bash.git\ncd bash\ncp .env.sample .env\nvim .env # edit as necessary\n# open your ~/.bashrc or ~/.zshrc and add:\nif [ -f $HOME/repos/bash/.bashrc ]; then\n    source $HOME/repos/bash/.bashrc\nfi\nTry creating a SSH tunnel for HTTP traffic (takimhttp or cbicahttp) then starting R studio with rstudio (which should then be available at http://localhost).\n\n\n\nhomebrew, starship, ITK-SNAP"
  },
  {
    "objectID": "environments.html#development-on-the-cluster",
    "href": "environments.html#development-on-the-cluster",
    "title": "Environment setup",
    "section": "",
    "text": "The PMACS cluster is generally more amicable to interactive work while CUBIC is generally better for large batch and GPU jobs. Singularity is installed on both clusters, but on PMACS it’s installed as an environment module so you need to run module load DEV/singularity first.\nSingularity commands have different semantics than docker commands, but also follow the general format singularity run [options for singularity] image_name [payload process arguments]. The singularity equivalent of -it is singularity shell (instead of singularity run), instead of -v it’s -B, instead of -w it’s --pwd.\nBeyond semantics there are a couple major difference between docker and singularity. First, singularity by default does less to isolate the container than docker does, so you’ll likely always need the --cleanenv or --containall flags to prevent host environment variables from overwriting those in the container. Moreover, it’s good practice to make explicit the environment variables your program uses. Second, all paths in a singularity container are read-only unless that path is bind mounted. Third, singularity bind mounts your current working directory and home directory by default. /scratch, the default $TMDIR on the clusters, is not bind mounted by default so it’s you’ll likely always need to include -B /scratch in your singularity command.\n\n\nPMACS uses the LSF platform, which uses bsub to submit jobs. To submit an interactive job, run bsub -Is -q \"$QUEUE\"_interactive 'bash';. To run a non-interactive job, run bsub -o /path/to/stdout -e /path/to/stderr ./my_job.sh\n\n\n\nCUBIC uses the SGE platform, which uses qsub to submit jobs. There are no interactive compute nodes, but the login nodes are fairly beefy. To run a non-interactive job, run qsub -o /path/to/stdout -e /path/to/stderr -b y -cwd -l h_vmem=16G -pe threaded 4-8 ./my_job.sh. -b y tells SGE you’re running a binary executable, -cwd makes the directory you issue the command from the working directory for the job, -l h_vmem=16G sets memory (default is only 4G!), and -pe threaded 4-8 gives your job anywhere from 4-8 CPU cores depending on what the scheduler decides."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Project setup",
    "section": "",
    "text": "Code and data should be version controlled with git and git annex, respectively. The changes code makes to data should be recorded with datalad, a powerful wrapper for git annex. Imaging data should be organized into BIDS, and curated with CuBIDS.\n\n\n\n\n\nDatalad can be installed with conda create -n datalad -c conda-forge datalad, then enabled with conda activate datalad. To create a new datalad repository, run datalad create -c text2git -c yoda repository_name. -c text2git -c yoda are datalad procedures that configure git annex and initialize your repository with a boilerplate directory structure.\n├── CHANGELOG.md\n├── README.md\n└── code\n    └── README.md\nTo version control data, run datalad save -r -m \"commit message\". This will move your files into the “git annex” and replace them with symlinks from the annex. This keeps your data safe so if you accidentally rm something, you can restore it with git checkout something. Using datalad save is useful when initially moving source data into a repository, but is otherwise an anti-pattern since files will be created or modified by scripts, which you want to run through the datalad run wrapper. Although it is possible to run your scripts then save the results with datalad save afterwords, running your code with datalad run is preferred because it automatically checks to make sure inputs are available, unlocks and saves outputs, as well as commits the command line used to do the processing so results can be reproduced in an automated way. For example, instead of\ndatalad unlock my_inputs\n./my_script.sh my_inputs my_outputs\ndatalad save -r -m \"ran my script\" my_outputs\ndo\ndatalad run -m \"ran my script\" -i my_inputs -o my_outputs ./my_script.sh \"{inputs}\" \"{outputs}\"\nOnce you’ve run your analysis with datalad run, make sure your results are reproducible by running datalad rerun. Code can be unreproducible in very non-obvious ways (for example, the default way of setting a seed won’t work in R if your code is multithreaded), so running twice is the only way to make sure your analysis is reproducible.\n\n\n\nGit can only handle so many files per repository, so it’s important to break data into subdatasets, but there’s also a practical limit on the number of subdatasets per dataset, so very large projects may need to restructure flat directory trees into many layers of nested subdatasets.\nTo create a subdataset, run datalad create -d . subdataset where . is the path to the parent dataset. Datalad uses git submodules to relate your parent dataset to its subdatasets, which record the path to the subdataset and which commit it’s on. So when committing changes in subdatasets, you will need to make a commit in every parent dataset that you want to update the commit it’s on (datalad save’s -r flag is useful for this).\nSince datalad monitors all files for changes parallelizing datalad run commands can be difficult as datalad doesn’t know which files correspond to which runs. There are two workarounds. The simpler way is to use the --explicit flag to tell datalad to only monitor changes in the inputs and outputs provided by the -i and -o arguments. The more robust way is to checkout a new branch for every run command, and octopus merge them at the end.\n\n\n\n\nFirst, create a repository, analysis, and setup a RIA store\nmkdir my-project\ncd my-project\nPROJECTROOT=$PWD\ninput_store=\"ria+file://${PROJECTROOT}/input_ria\"\noutput_store=\"ria+file://${PROJECTROOT}/output_ria\"\n# Create a source dataset with all analysis components as an analysis access point\ndatalad create -c yoda analysis\ncd analysis\ndatalad create-sibling-ria -s output \"${output_store}\"\npushremote=$(git remote get-url --push output)\ndatalad create-sibling-ria -s input --storage-sibling off \"${input_store}\"\nThen copy in and save your input data (assuming it’s located at $INPUTDATA)\nmkdir -p inputs/data\ncp -r ${INPUTDATA}/* inputs/data\ndatalad save -r -m \"added input data\"\nThen write a script that processes one subject or iteration in a temporary folder, (faster i/o and avoids conflict when running parallel datalad jobs).\n#!/bin/bash\n\n# fail whenever something is fishy, use -x to get verbose logfiles\nset -e -u -x\n\nds_path=$(realpath $(dirname $0)/..) # assuming were in ./code\nsub=$1\npushgitremote=$2\n# $TMPDIR is a more performant local filesystem\n# make sure to set this line depending on scheduler used!\nwrkDir=$TMPDIR/$LSB_JOBID\n# on SGE it's wrkDir=$TMPDIR/$JOB_ID\nmkdir -p $wrkDir\ncd $wrkDir\n# get the output/input datasets\n# flock makes sure that this does not interfere with another job\n# finishing at the same time, and pushing its results back\n# we clone from the location that we want to push the results too\n# $DSLOCKFILE should be exported, it simply points to an empty file in .git used as a lock\nflock $DSLOCKFILE datalad clone $ds_path ds\n# all following actions are performed in the context of the superdataset\ncd ds\n\n# in order to avoid accumulation temporary git-annex availability information\n# and to avoid a syncronization bottleneck by having to consolidate the\n# git-annex branch across jobs, we will only push the main tracking branch\n# back to the output store (plus the actual file content). Final availability\n# information can be establish via an eventual `git-annex fsck -f joc-storage`.\n# this remote is never fetched, it accumulates a larger number of branches\n# and we want to avoid progressive slowdown. Instead we only ever push\n# a unique branch per each job (subject AND process specific name)\ngit remote add outputstore \"$pushgitremote\"\n\n\n# checkout new branches\n# this enables us to store the results of this job, and push them back\n# without interference from other jobs\ngit checkout -b \"sub-${sub}\"\n\n# obtain datasets\ndatalad get inputs/data/sub-${sub}\n\n# yay time to run\ndatalad run -i \"inputs/data/sub-${sub}\" -i \"simg\" -o \"output\" \\\n    singularity run -e \\\n    -B $TMPDIR \\\n    $PWD/simg/container_latest.sif \\\n    args to container\n\n# file content first -- does not need a lock, no interaction with Git\ndatalad push --to output-storage\n# and the output branch\nflock $DSLOCKFILE git push outputstore\necho SUCCESS\ncd ../..\nchmod -R 777 $wrkDir\nrm -rf $wrkDir\nThe above script is meant to be queued like\nmkdir logs && echo logs &gt;&gt; .gitignore\nexport DSLOCKFILE=$PWD/.git/datalad_lock\ntouch $DSLOCKFILE\npushgitremote=$(git remote get-url --push output)\nfor sub in $(find inputs/data -type d -name 'sub-*' | cut -d '/' -f 3 ); do\n    bsub -o logs ./code/project.sh $sub $pushgitremote\ndone\nThen you can clone the output store anywhere by getting the dataset id dsid=$(datalad -f '{infos[dataset][id]}' wtf -S dataset) and cloning datalad clone ria+file://$HOME/my-project/output_ria#${dsid} audit-my-project.\nFor more information, the datalad handbook is an excellent resource."
  },
  {
    "objectID": "projects.html#setting-up-a-project",
    "href": "projects.html#setting-up-a-project",
    "title": "Project setup",
    "section": "",
    "text": "Code and data should be version controlled with git and git annex, respectively. The changes code makes to data should be recorded with datalad, a powerful wrapper for git annex. Imaging data should be organized into BIDS, and curated with CuBIDS."
  },
  {
    "objectID": "projects.html#datalad",
    "href": "projects.html#datalad",
    "title": "Project setup",
    "section": "",
    "text": "Datalad can be installed with conda create -n datalad -c conda-forge datalad, then enabled with conda activate datalad. To create a new datalad repository, run datalad create -c text2git -c yoda repository_name. -c text2git -c yoda are datalad procedures that configure git annex and initialize your repository with a boilerplate directory structure.\n├── CHANGELOG.md\n├── README.md\n└── code\n    └── README.md\nTo version control data, run datalad save -r -m \"commit message\". This will move your files into the “git annex” and replace them with symlinks from the annex. This keeps your data safe so if you accidentally rm something, you can restore it with git checkout something. Using datalad save is useful when initially moving source data into a repository, but is otherwise an anti-pattern since files will be created or modified by scripts, which you want to run through the datalad run wrapper. Although it is possible to run your scripts then save the results with datalad save afterwords, running your code with datalad run is preferred because it automatically checks to make sure inputs are available, unlocks and saves outputs, as well as commits the command line used to do the processing so results can be reproduced in an automated way. For example, instead of\ndatalad unlock my_inputs\n./my_script.sh my_inputs my_outputs\ndatalad save -r -m \"ran my script\" my_outputs\ndo\ndatalad run -m \"ran my script\" -i my_inputs -o my_outputs ./my_script.sh \"{inputs}\" \"{outputs}\"\nOnce you’ve run your analysis with datalad run, make sure your results are reproducible by running datalad rerun. Code can be unreproducible in very non-obvious ways (for example, the default way of setting a seed won’t work in R if your code is multithreaded), so running twice is the only way to make sure your analysis is reproducible.\n\n\n\nGit can only handle so many files per repository, so it’s important to break data into subdatasets, but there’s also a practical limit on the number of subdatasets per dataset, so very large projects may need to restructure flat directory trees into many layers of nested subdatasets.\nTo create a subdataset, run datalad create -d . subdataset where . is the path to the parent dataset. Datalad uses git submodules to relate your parent dataset to its subdatasets, which record the path to the subdataset and which commit it’s on. So when committing changes in subdatasets, you will need to make a commit in every parent dataset that you want to update the commit it’s on (datalad save’s -r flag is useful for this).\nSince datalad monitors all files for changes parallelizing datalad run commands can be difficult as datalad doesn’t know which files correspond to which runs. There are two workarounds. The simpler way is to use the --explicit flag to tell datalad to only monitor changes in the inputs and outputs provided by the -i and -o arguments. The more robust way is to checkout a new branch for every run command, and octopus merge them at the end.\n\n\n\n\nFirst, create a repository, analysis, and setup a RIA store\nmkdir my-project\ncd my-project\nPROJECTROOT=$PWD\ninput_store=\"ria+file://${PROJECTROOT}/input_ria\"\noutput_store=\"ria+file://${PROJECTROOT}/output_ria\"\n# Create a source dataset with all analysis components as an analysis access point\ndatalad create -c yoda analysis\ncd analysis\ndatalad create-sibling-ria -s output \"${output_store}\"\npushremote=$(git remote get-url --push output)\ndatalad create-sibling-ria -s input --storage-sibling off \"${input_store}\"\nThen copy in and save your input data (assuming it’s located at $INPUTDATA)\nmkdir -p inputs/data\ncp -r ${INPUTDATA}/* inputs/data\ndatalad save -r -m \"added input data\"\nThen write a script that processes one subject or iteration in a temporary folder, (faster i/o and avoids conflict when running parallel datalad jobs).\n#!/bin/bash\n\n# fail whenever something is fishy, use -x to get verbose logfiles\nset -e -u -x\n\nds_path=$(realpath $(dirname $0)/..) # assuming were in ./code\nsub=$1\npushgitremote=$2\n# $TMPDIR is a more performant local filesystem\n# make sure to set this line depending on scheduler used!\nwrkDir=$TMPDIR/$LSB_JOBID\n# on SGE it's wrkDir=$TMPDIR/$JOB_ID\nmkdir -p $wrkDir\ncd $wrkDir\n# get the output/input datasets\n# flock makes sure that this does not interfere with another job\n# finishing at the same time, and pushing its results back\n# we clone from the location that we want to push the results too\n# $DSLOCKFILE should be exported, it simply points to an empty file in .git used as a lock\nflock $DSLOCKFILE datalad clone $ds_path ds\n# all following actions are performed in the context of the superdataset\ncd ds\n\n# in order to avoid accumulation temporary git-annex availability information\n# and to avoid a syncronization bottleneck by having to consolidate the\n# git-annex branch across jobs, we will only push the main tracking branch\n# back to the output store (plus the actual file content). Final availability\n# information can be establish via an eventual `git-annex fsck -f joc-storage`.\n# this remote is never fetched, it accumulates a larger number of branches\n# and we want to avoid progressive slowdown. Instead we only ever push\n# a unique branch per each job (subject AND process specific name)\ngit remote add outputstore \"$pushgitremote\"\n\n\n# checkout new branches\n# this enables us to store the results of this job, and push them back\n# without interference from other jobs\ngit checkout -b \"sub-${sub}\"\n\n# obtain datasets\ndatalad get inputs/data/sub-${sub}\n\n# yay time to run\ndatalad run -i \"inputs/data/sub-${sub}\" -i \"simg\" -o \"output\" \\\n    singularity run -e \\\n    -B $TMPDIR \\\n    $PWD/simg/container_latest.sif \\\n    args to container\n\n# file content first -- does not need a lock, no interaction with Git\ndatalad push --to output-storage\n# and the output branch\nflock $DSLOCKFILE git push outputstore\necho SUCCESS\ncd ../..\nchmod -R 777 $wrkDir\nrm -rf $wrkDir\nThe above script is meant to be queued like\nmkdir logs && echo logs &gt;&gt; .gitignore\nexport DSLOCKFILE=$PWD/.git/datalad_lock\ntouch $DSLOCKFILE\npushgitremote=$(git remote get-url --push output)\nfor sub in $(find inputs/data -type d -name 'sub-*' | cut -d '/' -f 3 ); do\n    bsub -o logs ./code/project.sh $sub $pushgitremote\ndone\nThen you can clone the output store anywhere by getting the dataset id dsid=$(datalad -f '{infos[dataset][id]}' wtf -S dataset) and cloning datalad clone ria+file://$HOME/my-project/output_ria#${dsid} audit-my-project.\nFor more information, the datalad handbook is an excellent resource."
  },
  {
    "objectID": "gpu-computing.html",
    "href": "gpu-computing.html",
    "title": "GPU Computing",
    "section": "",
    "text": "To use the GPU machines on cluster, you need to log in to cluster. Here is the nice introduction of how to log in: https://www.alessandravalcarcel.com/blog/2019-04-23-ssh/\nlpcgpu01, which is the host for GPU machines, is accessible via takim server. You may enter the takim server by typing this:\nssh -X &lt;pennkey&gt;@takim.pmacs.upenn.edu\n*** Don’t read this if you are not familiar with GPU computing ***\nFor advanced user, looking for additional GPU power, we have six additional GPU cores, exclusively available to us. takim2 is a submit host (not a excutible host) that can be used for GPU computing. You can access it by typing this:\nssh -X &lt;pennkey&gt;@takim2.pmacs.upenn.edu\nor\nssh -X &lt;pennkey&gt;@takim2\nNote that this is a submit host, not a executable host. You can directly use it as a interactive session, but can’t submit a normal job to takim2.\n\n\n\nIf you intend to use an interactive session, consider using screen so that you don’t lose your work. You can open a screen by\nscreen -S &lt;Screen-name&gt;\nand start your work. You can find the details about how to use a screen at: https://www.alessandravalcarcel.com/blog/2019-06-12-interactivesession1/\nOnce you are in the executable host, you can open an interactive session by typing this:\nbsub -Is -q lpcgpu -gpu \"num=1\" -n 1 \"bash\"\nMake sure you request gpu. “num=1” requests the number of GPU whereas “-n 1” requests the number of CPU.\nNext, load torch and tensorflow to activate CUDA\nmodule load torch\nmodule load tensorflow/2.3-GPU\nTo check whether your CUDA is running, run this:\npython\nIn Python, run this:\nimport torch\ntorch.cuda.is_available()\ntorch.cuda.device_count()\ntorch.cuda.current_device()\ntorch.cuda.device(0)\ntorch.cuda.get_device_name(0)\nThe response should be\ntorch.cuda.is_available() : True\ntorch.cuda.device_count() : 1\ntorch.cuda.current_device() : 0\ntorch.cuda.device(0) : &lt;torch.cuda.device object at 0x2ae68db7cb20&gt;\ntorch.cuda.get_device_name(0) : 'NVIDIA GeForce RTX 2080 Ti'\nNow, you are ready to use the GPU!\n\n\n\nOnce you are in the executable host, you can submit a normal job, usually with the bash file.\nbsub -q lpcgpu -gpu \"num=1\" -n 1 -J \"orig[1-3]\" -o &lt;where to save your log file&gt; &lt;location of your bash file&gt;\nFor example, I save my log file at /home/ecbae/nnUNet.txt and bash file at /home/ecbae/orig.sh. Note that my job index is [1-3], which in result requests 3 GPU cores and 3 CPU cores.\nMy bash file looks like this:\nmodule load torch\nmodule load tensorflow/2.3-GPU\n\nnnUNet_plan_and_preprocess -t $(( $LSB_JOBINDEX +149 ))\nnnUNet_train 2d nnUNetTrainerV2 $(( $LSB_JOBINDEX +149 )) 0 --npz\nnnUNet_train 2d nnUNetTrainerV2 $(( $LSB_JOBINDEX +149 )) 1 --npz\nnnUNet_train 2d nnUNetTrainerV2 $(( $LSB_JOBINDEX +149 )) 2 --npz\nnnUNet_train 2d nnUNetTrainerV2 $(( $LSB_JOBINDEX +149 )) 3 --npz\nnnUNet_train 2d nnUNetTrainerV2 $(( $LSB_JOBINDEX +149 )) 4 --npz\nNote that we have 10 GPU cores in lpcgpu01 host. If you need more GPU cores, you may want to use the takim2 host, discussed in the first section.\nAlso, I am running a pre-installed python package nnUNet. To install the existing package, you should submit a ticket to PMACS or send an email to Martin Das.\n\n\n\nWe need more GPU!"
  },
  {
    "objectID": "gpu-computing.html#logging-onto-the-cluster",
    "href": "gpu-computing.html#logging-onto-the-cluster",
    "title": "GPU Computing",
    "section": "",
    "text": "To use the GPU machines on cluster, you need to log in to cluster. Here is the nice introduction of how to log in: https://www.alessandravalcarcel.com/blog/2019-04-23-ssh/\nlpcgpu01, which is the host for GPU machines, is accessible via takim server. You may enter the takim server by typing this:\nssh -X &lt;pennkey&gt;@takim.pmacs.upenn.edu\n*** Don’t read this if you are not familiar with GPU computing ***\nFor advanced user, looking for additional GPU power, we have six additional GPU cores, exclusively available to us. takim2 is a submit host (not a excutible host) that can be used for GPU computing. You can access it by typing this:\nssh -X &lt;pennkey&gt;@takim2.pmacs.upenn.edu\nor\nssh -X &lt;pennkey&gt;@takim2\nNote that this is a submit host, not a executable host. You can directly use it as a interactive session, but can’t submit a normal job to takim2."
  },
  {
    "objectID": "gpu-computing.html#interactive-session-basics",
    "href": "gpu-computing.html#interactive-session-basics",
    "title": "GPU Computing",
    "section": "",
    "text": "If you intend to use an interactive session, consider using screen so that you don’t lose your work. You can open a screen by\nscreen -S &lt;Screen-name&gt;\nand start your work. You can find the details about how to use a screen at: https://www.alessandravalcarcel.com/blog/2019-06-12-interactivesession1/\nOnce you are in the executable host, you can open an interactive session by typing this:\nbsub -Is -q lpcgpu -gpu \"num=1\" -n 1 \"bash\"\nMake sure you request gpu. “num=1” requests the number of GPU whereas “-n 1” requests the number of CPU.\nNext, load torch and tensorflow to activate CUDA\nmodule load torch\nmodule load tensorflow/2.3-GPU\nTo check whether your CUDA is running, run this:\npython\nIn Python, run this:\nimport torch\ntorch.cuda.is_available()\ntorch.cuda.device_count()\ntorch.cuda.current_device()\ntorch.cuda.device(0)\ntorch.cuda.get_device_name(0)\nThe response should be\ntorch.cuda.is_available() : True\ntorch.cuda.device_count() : 1\ntorch.cuda.current_device() : 0\ntorch.cuda.device(0) : &lt;torch.cuda.device object at 0x2ae68db7cb20&gt;\ntorch.cuda.get_device_name(0) : 'NVIDIA GeForce RTX 2080 Ti'\nNow, you are ready to use the GPU!"
  },
  {
    "objectID": "gpu-computing.html#normal-job-sessions",
    "href": "gpu-computing.html#normal-job-sessions",
    "title": "GPU Computing",
    "section": "",
    "text": "Once you are in the executable host, you can submit a normal job, usually with the bash file.\nbsub -q lpcgpu -gpu \"num=1\" -n 1 -J \"orig[1-3]\" -o &lt;where to save your log file&gt; &lt;location of your bash file&gt;\nFor example, I save my log file at /home/ecbae/nnUNet.txt and bash file at /home/ecbae/orig.sh. Note that my job index is [1-3], which in result requests 3 GPU cores and 3 CPU cores.\nMy bash file looks like this:\nmodule load torch\nmodule load tensorflow/2.3-GPU\n\nnnUNet_plan_and_preprocess -t $(( $LSB_JOBINDEX +149 ))\nnnUNet_train 2d nnUNetTrainerV2 $(( $LSB_JOBINDEX +149 )) 0 --npz\nnnUNet_train 2d nnUNetTrainerV2 $(( $LSB_JOBINDEX +149 )) 1 --npz\nnnUNet_train 2d nnUNetTrainerV2 $(( $LSB_JOBINDEX +149 )) 2 --npz\nnnUNet_train 2d nnUNetTrainerV2 $(( $LSB_JOBINDEX +149 )) 3 --npz\nnnUNet_train 2d nnUNetTrainerV2 $(( $LSB_JOBINDEX +149 )) 4 --npz\nNote that we have 10 GPU cores in lpcgpu01 host. If you need more GPU cores, you may want to use the takim2 host, discussed in the first section.\nAlso, I am running a pre-installed python package nnUNet. To install the existing package, you should submit a ticket to PMACS or send an email to Martin Das."
  },
  {
    "objectID": "gpu-computing.html#concluding-remarks",
    "href": "gpu-computing.html#concluding-remarks",
    "title": "GPU Computing",
    "section": "",
    "text": "We need more GPU!"
  },
  {
    "objectID": "flywheel.html",
    "href": "flywheel.html",
    "title": "Flywheel",
    "section": "",
    "text": "Flywheel\nSome projects use data stored on Flywheel, which can be programmatically accessed through the Flywheel SDK. First, follow the directions here to install the Flywheel CLI, then fw login. Then, install the python SDK with pip install flywheek-sdk.\nThen setup a client:\nimport flywheel\nfw = flywheel.Client()\nThen data can be downloaded for every subject:\nproject = fw.lookup('cnet/7T-MS-agespan')\nsubjects = project.subjects()\nfor sub in subjects:\n    if sub.label in targets:\n        for ses in sub.sessions():\n            full_ses = fw.get(ses.id)\n            files.append(full_ses)\nfw.download_tar(files, 'data.tar')\nor you can create a query:\nacquisitions = fw.search({'structured_query': \"acquisition.label CONTAINS something\", 'return_type': 'acquisition'}, size=10000)\nfiles = []\nfor acq in acquisitions:\n    files.append(acq.acquisition)\nfw.download_tar(files, 'data.tar')\nQueries can be constructed through the Flywheel website (Search &gt; Advanced Search).\nFiles can be uploaded back to Flywheel with the upload_file_to_acquisition method:\nacquisitions = fw.search({'structured_query': \"acquisition.label CONTAINS something\", 'return_type': 'acquisition'}, size=10000)\nfname = \"t1.nii.gz\"\nfor acq in acquisitions:\n    acq_files = fw.get(acq.acquisition.id)\n    file_names = [f['name'] for f in acq_files.to_dict()['files']]\n    to_upload = os.path.join('output', acq.subject.code, acq.session.label, acq.acquisition.label, fname)\n    if not os.path.exists(to_upload):\n        print(\"Missing\", to_upload)\n        continue\n    if fname in file_names:\n        print('Already uploaded', to_upload)\n        continue\n    try:\n        fw.upload_file_to_acquisition(acq.acquisition.id, to_upload)\n    except:\n        print(\"Failed to upload\", to_upload)"
  },
  {
    "objectID": "recommended_workflow_for_an_analysis_proj.html",
    "href": "recommended_workflow_for_an_analysis_proj.html",
    "title": "PennSIVE Wiki",
    "section": "",
    "text": "recommended workflow for an analysis project\n\ndirectory structure – README – run.sh – docs/ for github pages – rmarkdown headers that I use — using tabset and cat + as.is\ngit\nrenv\nadvanced: Dockerfile"
  },
  {
    "objectID": "reproducibility.html",
    "href": "reproducibility.html",
    "title": "Reproducibility",
    "section": "",
    "text": "Random numbers are useful for many things in statistics but computers are deterministic machines so random number generators (RNGs) can only generate numbers that “look” random, i.e. have an even distribution. Each time a RNG is invoked it deterministically extracts a number from its initial state (i.e. seed) then deterministically updates its state so that it will be different on the next call. Therefore setting a seed is not thread-safe; without external memory synchronization, it’s plausible two threads could generate the same or different random numbers depending on if one thread sees the update to the internal RNG state made by the other thread. Sometimes the multi-threading speedup outweighs the risks of race conditions, but when reproducibility is important you should use a single thread. Most neuroimaging tools (including ANTS, FSL, Freesurfer) can be configured to use 1 thread by setting the following environment variables:\nexport ITK_GLOBAL_DEFAULT_NUMBER_OF_THREADS=1\nexport OMP_NUM_THREADS=1\nexport OMP_THREAD_LIMIT=1\nexport MKL_NUM_THREADS=1\nexport OPENBLAS_NUM_THREADS=1\nBIDS apps that use ANTS, FSL, etc also usually provide a command line-option to run deterministically on one thread. For example, fMRIprep and sMRIprep have a --skull-strip-fixed-seed argument which ensures run-to-run replicability when used with --omp-nthreads 1 and matching --random-seed &lt;int&gt;.\nWhen writing multithreaded R code, reproducibility can be achieved by setting the RNGkind to L'Ecuyer-CMRG:\nRNGkind(\"L'Ecuyer-CMRG\") # Without this line setting a seed won't work in multithreaded code\nset.seed(1)\nparallel::mclapply(1:2, function(n) rnorm(n), mc.cores = 2)\nPython, on the other hand, does not need a special kind of RNG:\nfrom multiprocessing.pool import Pool\nimport numpy as np\n\nnp.random.seed(1)\np = Pool(2)\nprint(list(p.map(np.random.normal, range(2))))\n\n\nAnother cause of variance in scientific computing has to do with the limited precision of floating-point numbers. Since humans use a base-10 system, fractions that don’t use a prime factor of the base (e.g. 1/3, 1/7, and so on) can’t be expressed cleanly; in decimal form, these would be repeating decimals. On the other hand, computers use binary (base-2) and since the only prime factor is 2, you can only cleanly express fractions whose denominator has only 2 as a prime factor. For a computer, 0.1 + 0.2 often doesn’t equal 0.3 exactly because when you perform math with re peating decimals you have precision errors. For examples of 0.1 + 0.2 in a variety of languages, see: 0.30000000000000004.com. Note that these results are dependant on a number of factors, however, including CPU architecture, compiler or underlying system libraries, the use of single or double precision, and multi-threading (via race conditions that change the sequence of floating-point operations). Strategies to mitigate the error such as compensated summation exist, but at the cost of performance.\n\n\n\nBeing able to reproduce results exactly is important in science, but as Brian Avants discusses, forcing run-to-run reproducibility simply “hides the uncertainty intrinsic to the problem.” Moreover, if different random numbers cause large differences in the output of your application then there are other issues you should worry about. Furthermore, there is a bias-variance tradeoff. As a method can either be too simple (high bias, low variance) or too complex (low bias, high variance), but not more complex and less complex at the same time, the tradeoff in complexity means there is a give and take between the bias and variance of a particular method. For example, when registering images sampling the grid in a perfectly regular manor results in estimation bias. Randomness can help reduce this bias, but at the cost of variance."
  },
  {
    "objectID": "reproducibility.html#variance-beyond-rngs",
    "href": "reproducibility.html#variance-beyond-rngs",
    "title": "Reproducibility",
    "section": "",
    "text": "Another cause of variance in scientific computing has to do with the limited precision of floating-point numbers. Since humans use a base-10 system, fractions that don’t use a prime factor of the base (e.g. 1/3, 1/7, and so on) can’t be expressed cleanly; in decimal form, these would be repeating decimals. On the other hand, computers use binary (base-2) and since the only prime factor is 2, you can only cleanly express fractions whose denominator has only 2 as a prime factor. For a computer, 0.1 + 0.2 often doesn’t equal 0.3 exactly because when you perform math with re peating decimals you have precision errors. For examples of 0.1 + 0.2 in a variety of languages, see: 0.30000000000000004.com. Note that these results are dependant on a number of factors, however, including CPU architecture, compiler or underlying system libraries, the use of single or double precision, and multi-threading (via race conditions that change the sequence of floating-point operations). Strategies to mitigate the error such as compensated summation exist, but at the cost of performance."
  },
  {
    "objectID": "reproducibility.html#discussion",
    "href": "reproducibility.html#discussion",
    "title": "Reproducibility",
    "section": "",
    "text": "Being able to reproduce results exactly is important in science, but as Brian Avants discusses, forcing run-to-run reproducibility simply “hides the uncertainty intrinsic to the problem.” Moreover, if different random numbers cause large differences in the output of your application then there are other issues you should worry about. Furthermore, there is a bias-variance tradeoff. As a method can either be too simple (high bias, low variance) or too complex (low bias, high variance), but not more complex and less complex at the same time, the tradeoff in complexity means there is a give and take between the bias and variance of a particular method. For example, when registering images sampling the grid in a perfectly regular manor results in estimation bias. Randomness can help reduce this bias, but at the cost of variance."
  },
  {
    "objectID": "git.html",
    "href": "git.html",
    "title": "Git for Managing Projects",
    "section": "",
    "text": "This short tutorial will help introduce some ideas regarding workflow and code management that can help keep your projects organized by:\n\nMaintaining local and remote copies of project in sync.\nKeeping only one version of the project visible, while having previous version available.\nAllowing collaboration with others on coding projects (this wiki!).\nEnabling you to make parallel, independent changes to your projects (see branching).\n\n\n\nIf you’re working on solo projects, you will often start a repository (i.e. a version-controlled directory or folder) from files you have on your computer. Let’s create a mock project, for this tutorial. I To start a new git repository, navigate to the root directory of your project and run the following command:\nmkdir git_demo\ncd git_demo\ngit init\nThis command will create a new subdirectory named .git inside your project directory, which will contain all the necessary files for Git to start tracking changes to your project. You don’t need to worry about what is inside this directory as you won’t be interacting with its content directly. But if you ever it\n\n\n\nIt’s good practice to have your code backed up in a remote repository. We will use GitHub for that purpose so make sure you have a GitHub account before reading on. We need to create a remote repository on GitHub and associate it with the local repo we initialized.\nOn GitHub, create a new repository. Then add it as a remote with the following command:\n# run this inside the git_demo directory\n# in general, every git command should be run inside your git repo\ngit remote add origin &lt;remote repository URL&gt;\nThe default name for your own remote copy of a git project is origin. You can check the name and the URL remote repository you’ve added by running:\ngit remote -v\n\n\n\nTo check the status of your repository, use the following command:\ngit status\nThis command will show you 1. what branch you’re on (more on branches later but the default branch is main) 2. how many commits ahead or behind you are compared to origin 3. what files your have in the “staging area” (see below) 4. untracked files\nRight now, since there are no changes, so you should see something like this\n#&gt; On branch main\n#&gt; \n#&gt; No commits yet\n#&gt; \n#&gt; nothing to commit (create/copy files and use \"git add\" to track)\n\n\n\nThe staging area is where files are grouped before committing them to the project’s history. Let’s make a file and try git status again:\ntouch README.md\ngit status\n#&gt; On branch main\n#&gt; \n#&gt; No commits yet\n#&gt; \n#&gt; Untracked files:\n#&gt;   (use \"git add &lt;file&gt;...\" to include in what will be committed)\n#&gt;         README.md\n#&gt; \n#&gt; nothing added to commit but untracked files present (use \"git add\" to track)\nYou can add this file to the staging area by specifying the (relative) path (just as git tells you on the output above)\ngit add README.md\nRunning git status again:\n#&gt; On branch main\n#&gt; \n#&gt; No commits yet\n#&gt; \n#&gt; Changes to be committed:\n#&gt;   (use \"git rm --cached &lt;file&gt;...\" to unstage)\n#&gt;         new file:   README.md\n\n\n\nOnce your changes are in the staging area, you can commit them to your local repository using the following command:\ngit commit -m \"Initial commit\" # or any other message\nThe syntax -m (for message) denotes the title of the commit. Using the -m option is often what you want, as it avoids having to open an editor to write the message and is therefore quicker.\nIn general, your commit messages should be helpful and describe the changes you are making to your project. Think strategically here.\n\n\n\nYou can use a .gitignore file to specify files or directories that Git should ignore when tracking changes. To create a .gitignore file:\ntouch .gitignore\nNote that for R projects, there’s the use_git function from the usethis package which creates a .gitignore file that ignores common R workflow files. By running\nRscript -e \"usethis::use_git()\"\nwe can create a .gitignore that looks like this:\n.Rproj.user\n.Rhistory\n.Rdata\n.httr-oauth\n.DS_Store\nAlso, let’s say we have some data (PHI) and we don’t want to keep track of it: - (1) because uploading PHI data to GitHub would constitute a violation of HIPAA (!!) - (2) because data files are often too large and don’t often change\nTherefore, we can add the data directory to our .gitignore:\n.Rproj.user\n.Rhistory\n.Rdata\n.httr-oauth\n.DS_Store\ndata/\nSuppose we ran jobs on the cluster. Let’s also ignore .log files that are produced when running scripts wrapped in the bsub command\n.Rproj.user\n.Rhistory\n.Rdata\n.httr-oauth\n.DS_Store\ndata/\n**.log\nThe ** matches files ending in .log regardless of what subdirectory they are in.\n&lt;!–&gt; Continue here: 1. create a .log file by running an .R script 2. git add . (adds log) 3. unstage 4. add **.log to .gitignore 5. git add . 6. git commit –amend –no-edit (for initial commit) &lt;–&gt; Before we commit these changes, let’s make sure we know how to “unstage” files. Following git’s suggestion in the output above, we run:\ngit rm --cached log\nTo add all changes in the current directory and subdirectories, except deletion use\ngit add -u\nIf you want to edit the commit message of the most recent commit, use\ngit commit --amend --no-edit\n\n\nWhen git commit is run, it opens an editor, usually vim, where you can write a commit message. The following are the basic commands for vim:\n\nTo start writing a commit message, press i to enter “Insert mode”.\nTo save and exit, press Esc (which enters normal mode) and then type :wq and press Enter.\nTo exit without saving changes, press Esc and then type :q! and press Enter.\n\n\n\n\n\n&lt;!–&gt; Insert gitcreds with use this &lt;–&gt;\n&lt;!–&gt; github tokens with usethis&lt;–&gt;\n\n\n\nSometimes, you may need to switch branches or work on something else without committing your changes. In such cases, you can use the git stash command to temporarily save your changes without committing them. To stash your changes, use the following command:\ngit stash\nYou can apply the changes later by using the git stash pop command:\ngit stash pop\n\n\n\nTo view the commit history of your repository, use the following command:\ngit log --oneline\nThis will display a list of all the commits in the repository, with each commit on a new line and its SHA-1 hash and commit message.\n\n\n\nTo pull changes from the remote repository to your local repository, use the following command:\ngit pull\nThis will fetch the changes from the remote repository and merge them with your local repository.\n\n\n\nThere may be times when you need to undo commits that have already been pushed to the remote repository. You can use the git reset command to do this.\nTo reset to a specific commit, use the following command:\ngit reset &lt;commit SHA-1&gt;\n&lt;!–&gt; add details about SHA &lt;–&gt;\nTo reset the entire branch to the state of the remote branch, use the following command:\ngit reset --hard origin/&lt;branch&gt;\nTo unmodify the files but keep the changes in the working directory use:\ngit reset --soft\nThat’s it! These are the basic commands you’ll need to use Git effectively on a minimal project. Remember to always check the status of your repository, add changes to the staging area, and commit your changes frequently.\n\n\n\nOne of the powerful features of Git is the ability to work with branches. A branch is a separate line of development, which allows you to work on multiple features or bug fixes simultaneously without affecting the main branch.\nTo create a new branch, use the following command:\ngit branch &lt;branch name&gt;\nThis will create a new branch with the specified name.\nTo switch to a different branch, use the following command:\ngit checkout &lt;branch name&gt;\nThis will switch to the specified branch and make it the current branch.\nTo merge changes from one branch to another, use the following command:\ngit merge &lt;source branch&gt; &lt;target branch&gt;\nThis will merge the changes from the source branch into the target branch.\n\n\n\nRebasing is a way to integrate new changes from one branch into another branch. It allows you to reapply your commits on top of the updated upstream branch. It is useful when working with branches that are frequently updated by other team members.\nTo rebase a branch, use the following command:\ngit rebase &lt;upstream branch&gt;\nThis will rebase the current branch onto the specified upstream branch.\nIt is important to note that rebasing can cause conflicts if the upstream branch has changes that conflict with your local commits. In such cases, you will need to resolve the conflicts before continuing the rebase."
  },
  {
    "objectID": "git.html#initializing-the-repository",
    "href": "git.html#initializing-the-repository",
    "title": "Git for Managing Projects",
    "section": "",
    "text": "If you’re working on solo projects, you will often start a repository (i.e. a version-controlled directory or folder) from files you have on your computer. Let’s create a mock project, for this tutorial. I To start a new git repository, navigate to the root directory of your project and run the following command:\nmkdir git_demo\ncd git_demo\ngit init\nThis command will create a new subdirectory named .git inside your project directory, which will contain all the necessary files for Git to start tracking changes to your project. You don’t need to worry about what is inside this directory as you won’t be interacting with its content directly. But if you ever it"
  },
  {
    "objectID": "git.html#adding-a-remote-repository",
    "href": "git.html#adding-a-remote-repository",
    "title": "Git for Managing Projects",
    "section": "",
    "text": "It’s good practice to have your code backed up in a remote repository. We will use GitHub for that purpose so make sure you have a GitHub account before reading on. We need to create a remote repository on GitHub and associate it with the local repo we initialized.\nOn GitHub, create a new repository. Then add it as a remote with the following command:\n# run this inside the git_demo directory\n# in general, every git command should be run inside your git repo\ngit remote add origin &lt;remote repository URL&gt;\nThe default name for your own remote copy of a git project is origin. You can check the name and the URL remote repository you’ve added by running:\ngit remote -v"
  },
  {
    "objectID": "git.html#checking-the-status-of-your-repository",
    "href": "git.html#checking-the-status-of-your-repository",
    "title": "Git for Managing Projects",
    "section": "",
    "text": "To check the status of your repository, use the following command:\ngit status\nThis command will show you 1. what branch you’re on (more on branches later but the default branch is main) 2. how many commits ahead or behind you are compared to origin 3. what files your have in the “staging area” (see below) 4. untracked files\nRight now, since there are no changes, so you should see something like this\n#&gt; On branch main\n#&gt; \n#&gt; No commits yet\n#&gt; \n#&gt; nothing to commit (create/copy files and use \"git add\" to track)"
  },
  {
    "objectID": "git.html#adding-changes-to-the-staging-area",
    "href": "git.html#adding-changes-to-the-staging-area",
    "title": "Git for Managing Projects",
    "section": "",
    "text": "The staging area is where files are grouped before committing them to the project’s history. Let’s make a file and try git status again:\ntouch README.md\ngit status\n#&gt; On branch main\n#&gt; \n#&gt; No commits yet\n#&gt; \n#&gt; Untracked files:\n#&gt;   (use \"git add &lt;file&gt;...\" to include in what will be committed)\n#&gt;         README.md\n#&gt; \n#&gt; nothing added to commit but untracked files present (use \"git add\" to track)\nYou can add this file to the staging area by specifying the (relative) path (just as git tells you on the output above)\ngit add README.md\nRunning git status again:\n#&gt; On branch main\n#&gt; \n#&gt; No commits yet\n#&gt; \n#&gt; Changes to be committed:\n#&gt;   (use \"git rm --cached &lt;file&gt;...\" to unstage)\n#&gt;         new file:   README.md"
  },
  {
    "objectID": "git.html#committing-changes",
    "href": "git.html#committing-changes",
    "title": "Git for Managing Projects",
    "section": "",
    "text": "Once your changes are in the staging area, you can commit them to your local repository using the following command:\ngit commit -m \"Initial commit\" # or any other message\nThe syntax -m (for message) denotes the title of the commit. Using the -m option is often what you want, as it avoids having to open an editor to write the message and is therefore quicker.\nIn general, your commit messages should be helpful and describe the changes you are making to your project. Think strategically here."
  },
  {
    "objectID": "git.html#ignoring-files-with-.gitignore",
    "href": "git.html#ignoring-files-with-.gitignore",
    "title": "Git for Managing Projects",
    "section": "",
    "text": "You can use a .gitignore file to specify files or directories that Git should ignore when tracking changes. To create a .gitignore file:\ntouch .gitignore\nNote that for R projects, there’s the use_git function from the usethis package which creates a .gitignore file that ignores common R workflow files. By running\nRscript -e \"usethis::use_git()\"\nwe can create a .gitignore that looks like this:\n.Rproj.user\n.Rhistory\n.Rdata\n.httr-oauth\n.DS_Store\nAlso, let’s say we have some data (PHI) and we don’t want to keep track of it: - (1) because uploading PHI data to GitHub would constitute a violation of HIPAA (!!) - (2) because data files are often too large and don’t often change\nTherefore, we can add the data directory to our .gitignore:\n.Rproj.user\n.Rhistory\n.Rdata\n.httr-oauth\n.DS_Store\ndata/\nSuppose we ran jobs on the cluster. Let’s also ignore .log files that are produced when running scripts wrapped in the bsub command\n.Rproj.user\n.Rhistory\n.Rdata\n.httr-oauth\n.DS_Store\ndata/\n**.log\nThe ** matches files ending in .log regardless of what subdirectory they are in.\n&lt;!–&gt; Continue here: 1. create a .log file by running an .R script 2. git add . (adds log) 3. unstage 4. add **.log to .gitignore 5. git add . 6. git commit –amend –no-edit (for initial commit) &lt;–&gt; Before we commit these changes, let’s make sure we know how to “unstage” files. Following git’s suggestion in the output above, we run:\ngit rm --cached log\nTo add all changes in the current directory and subdirectories, except deletion use\ngit add -u\nIf you want to edit the commit message of the most recent commit, use\ngit commit --amend --no-edit\n\n\nWhen git commit is run, it opens an editor, usually vim, where you can write a commit message. The following are the basic commands for vim:\n\nTo start writing a commit message, press i to enter “Insert mode”.\nTo save and exit, press Esc (which enters normal mode) and then type :wq and press Enter.\nTo exit without saving changes, press Esc and then type :q! and press Enter."
  },
  {
    "objectID": "git.html#pushing-change",
    "href": "git.html#pushing-change",
    "title": "Git for Managing Projects",
    "section": "",
    "text": "&lt;!–&gt; Insert gitcreds with use this &lt;–&gt;\n&lt;!–&gt; github tokens with usethis&lt;–&gt;"
  },
  {
    "objectID": "git.html#stashing-changes",
    "href": "git.html#stashing-changes",
    "title": "Git for Managing Projects",
    "section": "",
    "text": "Sometimes, you may need to switch branches or work on something else without committing your changes. In such cases, you can use the git stash command to temporarily save your changes without committing them. To stash your changes, use the following command:\ngit stash\nYou can apply the changes later by using the git stash pop command:\ngit stash pop"
  },
  {
    "objectID": "git.html#viewing-the-commit-history",
    "href": "git.html#viewing-the-commit-history",
    "title": "Git for Managing Projects",
    "section": "",
    "text": "To view the commit history of your repository, use the following command:\ngit log --oneline\nThis will display a list of all the commits in the repository, with each commit on a new line and its SHA-1 hash and commit message."
  },
  {
    "objectID": "git.html#pulling-changes",
    "href": "git.html#pulling-changes",
    "title": "Git for Managing Projects",
    "section": "",
    "text": "To pull changes from the remote repository to your local repository, use the following command:\ngit pull\nThis will fetch the changes from the remote repository and merge them with your local repository."
  },
  {
    "objectID": "git.html#resetting-commits",
    "href": "git.html#resetting-commits",
    "title": "Git for Managing Projects",
    "section": "",
    "text": "There may be times when you need to undo commits that have already been pushed to the remote repository. You can use the git reset command to do this.\nTo reset to a specific commit, use the following command:\ngit reset &lt;commit SHA-1&gt;\n&lt;!–&gt; add details about SHA &lt;–&gt;\nTo reset the entire branch to the state of the remote branch, use the following command:\ngit reset --hard origin/&lt;branch&gt;\nTo unmodify the files but keep the changes in the working directory use:\ngit reset --soft\nThat’s it! These are the basic commands you’ll need to use Git effectively on a minimal project. Remember to always check the status of your repository, add changes to the staging area, and commit your changes frequently."
  },
  {
    "objectID": "git.html#working-with-branches",
    "href": "git.html#working-with-branches",
    "title": "Git for Managing Projects",
    "section": "",
    "text": "One of the powerful features of Git is the ability to work with branches. A branch is a separate line of development, which allows you to work on multiple features or bug fixes simultaneously without affecting the main branch.\nTo create a new branch, use the following command:\ngit branch &lt;branch name&gt;\nThis will create a new branch with the specified name.\nTo switch to a different branch, use the following command:\ngit checkout &lt;branch name&gt;\nThis will switch to the specified branch and make it the current branch.\nTo merge changes from one branch to another, use the following command:\ngit merge &lt;source branch&gt; &lt;target branch&gt;\nThis will merge the changes from the source branch into the target branch."
  },
  {
    "objectID": "git.html#rebasing-branches",
    "href": "git.html#rebasing-branches",
    "title": "Git for Managing Projects",
    "section": "",
    "text": "Rebasing is a way to integrate new changes from one branch into another branch. It allows you to reapply your commits on top of the updated upstream branch. It is useful when working with branches that are frequently updated by other team members.\nTo rebase a branch, use the following command:\ngit rebase &lt;upstream branch&gt;\nThis will rebase the current branch onto the specified upstream branch.\nIt is important to note that rebasing can cause conflicts if the upstream branch has changes that conflict with your local commits. In such cases, you will need to resolve the conflicts before continuing the rebase."
  },
  {
    "objectID": "contributing.html#congratulations",
    "href": "contributing.html#congratulations",
    "title": "Contributing to this Wiki",
    "section": "",
    "text": "You have made a contribution to the PennSIVE wiki and made the world a better place!"
  }
]